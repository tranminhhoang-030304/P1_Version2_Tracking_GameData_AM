from etl_processor import run_etl_pipeline
from flask import Flask, jsonify, request
from flask_cors import CORS
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime, timedelta
import time
import requests
import threading
import random

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# --- C·∫§U H√åNH DATABASE ---
DB_HOST = "localhost"
DB_NAME = "p1_gamedata"
DB_USER = "postgres"
DB_PASS = "Rose24092001" # Password c·ªßa b·∫°n

# --- C·∫§U H√åNH H·ªÜ TH·ªêNG ---
MAX_RETRIES = 18       # S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa cho Auto Worker

# --- TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG TO√ÄN C·ª§C ---
SYSTEM_STATE = {
    "is_busy": False,          
    "current_app_id": None,    
    "current_run_type": None   
}
SYSTEM_LOCK = threading.Lock() 
JOB_STOP_EVENTS = {}

def smart_parse_json(raw_input):
    """
    H√†m th√¥ng minh ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªùng h·ª£p JSON b·ªã l·ªìng 2 l·ªõp.
    V√≠ d·ª•: "{\"event_json\": \"{\\\"levelID\\\": 1...}\"}"
    """
    if not raw_input: 
        return {}
    
    try:
        # L·ªõp 1: N·∫øu l√† string th√¨ parse ra dict, n·∫øu l√† dict r·ªìi th√¨ gi·ªØ nguy√™n
        parsed_data = json.loads(raw_input) if isinstance(raw_input, str) else raw_input
        
        # L·ªõp 2: Ki·ªÉm tra xem b√™n trong c√≥ key 'event_json' ch·ª©a string JSON n·ªØa kh√¥ng (L·ªói double encode)
        if isinstance(parsed_data, dict) and 'event_json' in parsed_data:
            inner_value = parsed_data['event_json']
            if isinstance(inner_value, str):
                try:
                    inner_json = json.loads(inner_value)
                    # G·ªôp d·ªØ li·ªáu b√™n trong ra ngo√†i (Flatten)
                    parsed_data.update(inner_json)
                except:
                    pass # N·∫øu kh√¥ng parse ƒë∆∞·ª£c l·ªõp trong th√¨ th√¥i
                    
        return parsed_data
    except Exception:
        return {}

def get_db():
    try:
        conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS)
        return conn
    except Exception as e:
        print("‚ùå L·ªñI K·∫æT N·ªêI DB:", e)
        return None

# --- H√ÄM QU·∫¢N L√ù TR·∫†NG TH√ÅI B·∫¨N/R·∫¢NH ---
def set_system_busy(busy, app_id=None, run_type=None):
    with SYSTEM_LOCK:
        SYSTEM_STATE["is_busy"] = busy
        SYSTEM_STATE["current_app_id"] = app_id
        SYSTEM_STATE["current_run_type"] = run_type

def is_system_busy():
    with SYSTEM_LOCK:
        return SYSTEM_STATE["is_busy"]

# ==========================================
# PH·∫¶N 1: CORE FUNCTIONS (T·∫†O JOB & C·∫¨P NH·∫¨T)
# ==========================================

def create_etl_job(app_id, date_since, date_until):
    conn = get_db()
    if not conn: return
    cur = conn.cursor()
    try:
        # Check tr√πng: N·∫øu ƒë√£ c√≥ job ƒëang ch·ªù/ch·∫°y c√πng khung gi·ªù th√¨ th√¥i
        cur.execute("""
            SELECT id FROM etl_jobs 
            WHERE app_id = %s AND date_since = %s AND status IN ('pending', 'processing')
        """, (app_id, date_since))
        if cur.fetchone(): 
            return # ƒê√£ c√≥ job r·ªìi, kh√¥ng t·∫°o th√™m

        cur.execute("""
            INSERT INTO etl_jobs (app_id, date_since, date_until, status, retry_count, message, created_at)
            VALUES (%s, %s, %s, 'pending', 0, 'Scheduled Auto', NOW())
        """, (app_id, date_since, date_until))
        conn.commit()
        print(f"üé´ Auto: ƒê√£ t·∫°o v√© Job cho App {app_id}")
    except Exception as e:
        print(f"‚ùå Auto Error: {e}")
    finally:
        cur.close()
        conn.close()

def update_job_status(job_id, status, message=None, inc_retry=False):
    conn = get_db()
    if not conn: return
    cur = conn.cursor()
    try:
        sql = "UPDATE etl_jobs SET status = %s, updated_at = NOW(), message = %s"
        if inc_retry: sql += ", retry_count = retry_count + 1"
        sql += " WHERE id = %s"
        cur.execute(sql, (status, message, job_id))
        conn.commit()
    finally:
        cur.close()
        conn.close()

# ==========================================
# PH·∫¶N 2: WORKER TH√îNG MINH (ƒê√É S·ª¨A L·ªñI TIME & INSERT TR∆Ø·ªöC)
# ==========================================
# --- H√ÄM PH·ª§ TR·ª¢: GHI LOG V√ÄO DB ---
def append_log_to_db(hist_id, new_log_line):
    """N·ªëi th√™m log v√†o d√≤ng l·ªãch s·ª≠ ƒëang ch·∫°y"""   
    if not hist_id: return
    try:
        conn = get_db()
        cur = conn.cursor()
        # D√πng to√°n t·ª≠ || ƒë·ªÉ n·ªëi chu·ªói trong PostgreSQL
        timestamp = datetime.now().strftime('%H:%M:%S')
        log_entry = f"\n[{timestamp}] {new_log_line}"
        
        cur.execute("""
            UPDATE job_history 
            SET logs = COALESCE(logs, '') || %s, updated_at = NOW()
            WHERE id = %s
        """, (log_entry, hist_id))
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"‚ùå Error appending log: {e}")

def transform_events_to_level_analytics(app_id, events):
    """
    [UPDATED] Transform missionStart / missionComplete / missionFail
    S·ª≠ d·ª•ng smart_parse_json ƒë·ªÉ x·ª≠ l√Ω l·ªói l·ªìng JSON.
    """
    if not events:
        return

    conn = get_db()
    cur = conn.cursor()

    # 1. Gom event theo (user_id, level_id)
    sessions = {}

    for e in events:
        try:
            event_name = e.get("event_name")
            
            # --- [S·ª¨A ƒê·ªîI QUAN TR·ªåNG T·∫†I ƒê√ÇY] ---
            # Thay v√¨ json.loads ƒë∆°n thu·∫ßn, ta d√πng h√†m th√¥ng minh
            raw_json = smart_parse_json(e.get("event_json"))
            # ------------------------------------

            level_id = raw_json.get("levelID") or raw_json.get("missionID")
            user_id = raw_json.get("userID") or "Guest"
            
            # N·∫øu v·∫´n kh√¥ng l·∫•y ƒë∆∞·ª£c level_id, b·ªè qua event n√†y
            if not level_id:
                continue

            session_key = f"{user_id}_{level_id}"
            
            # Timestamp x·ª≠ l√Ω an to√†n
            try:
                ts_val = int(e.get("event_timestamp"))
                ts = datetime.fromtimestamp(ts_val)
            except:
                ts = datetime.now()

            if session_key not in sessions:
                sessions[session_key] = {
                    "app_id": app_id,
                    "user_id": user_id,
                    "level_id": level_id,
                    "start_time": None,
                    "end_time": None,
                    "status": "DROP",
                    "total_cost": 0
                }

            s = sessions[session_key]

            # Logic t√≠nh to√°n gi·ªØ nguy√™n, ch·ªâ ƒë·∫£m b·∫£o raw_json ƒë√£ s·∫°ch
            if event_name == "missionStart":
                s["start_time"] = ts

            elif event_name == "missionComplete":
                s["end_time"] = ts
                s["status"] = "WIN"
                for k, v in raw_json.items():
                    if k.startswith("booster_") or k.startswith("revive_"):
                        try: s["total_cost"] += int(v)
                        except: pass

            elif event_name == "missionFail":
                s["end_time"] = ts
                s["status"] = "FAIL"
                for k, v in raw_json.items():
                    if k.startswith("booster_") or k.startswith("revive_"):
                        try: s["total_cost"] += int(v)
                        except: pass

        except Exception as ex:
            print(f"Transform error skipping row: {ex}")

    # 2. Insert v√†o level_analytics
    for s in sessions.values():
        start_time = s["start_time"]
        end_time = s["end_time"]
        duration = 0
        if start_time and end_time:
            duration = int((end_time - start_time).total_seconds())

        # Ch·ªâ insert n·∫øu c√≥ d·ªØ li·ªáu h·ª£p l·ªá (Tr√°nh r√°c)
        try:
            cur.execute("""
                INSERT INTO level_analytics
                (app_id, session_id, user_id, level_name, status, duration, start_time, total_cost, created_at)
                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,NOW())
            """, (
                s["app_id"],
                f"{s['user_id']}_{s['level_id']}",
                s["user_id"],
                f"Level {s['level_id']}", # Format t√™n Level ƒë·∫πp h∆°n
                s["status"],
                duration,
                start_time,
                s["total_cost"]
            ))
        except Exception as insert_err:
            print(f"Insert Analytics Error: {insert_err}")

    conn.commit()
    conn.close()

def worker_process_jobs():
    if is_system_busy(): return

    conn = get_db()
    if not conn: return
    
    # 1. L·∫•y Job ƒëang ch·ªù
    cur = conn.cursor(cursor_factory=RealDictCursor)
    cur.execute("""
        SELECT * FROM etl_jobs 
        WHERE status IN ('pending', 'processing') 
        ORDER BY created_at ASC LIMIT 1
    """)
    job = cur.fetchone()
    cur.close() 
    conn.close()

    if not job: return

    job_id = job['id']
    app_id = job['app_id']
    retry_count = job['retry_count']

    # ================= [B·∫ÆT ƒê·∫¶U ƒêO·∫†N C·∫¶N TH√äM] =================
    # LOGIC: N·∫øu th·∫•y l√† Retry, t·ª± ƒë·ªông l·ªôi ng∆∞·ª£c d√≤ng t√¨m ng√†y c≈©
    # (Ghi ƒë√® l·∫°i date_since/date_until m√† API ƒë√£ t√≠nh sai)
    if job.get('run_type') == 'retry' and job.get('retry_job_id'):
        try:
            print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Worker ph√°t hi·ªán Retry cho Job #{job['retry_job_id']}. ƒêang t√≠nh l·∫°i ng√†y...")
            conn_fix = get_db()
            cur_fix = conn_fix.cursor(cursor_factory=RealDictCursor)
            
            # L·∫•y gi·ªù ch·∫°y c·ªßa Job qu√° kh·ª©
            cur_fix.execute("SELECT start_time FROM job_history WHERE id = %s", (job['retry_job_id'],))
            old_job = cur_fix.fetchone()
            cur_fix.close()
            conn_fix.close()

            if old_job and old_job['start_time']:
                # T√≠nh l·∫°i c·ª≠a s·ªï th·ªùi gian (gi·ªëng logic chu k·ª≥ 1 ti·∫øng)
                fix_target = old_job['start_time']
                fix_from = fix_target - timedelta(minutes=65)
                
                # C·∫¨P NH·∫¨T L·∫†I D·ªÆ LI·ªÜU TRONG B·ªò NH·ªö
                job['date_until'] = fix_target.strftime('%Y-%m-%d %H:%M:%S')
                job['date_since'] = fix_from.strftime('%Y-%m-%d %H:%M:%S')
                print(f"‚úÖ ƒê√£ ƒëi·ªÅu ch·ªânh th·ªùi gian v·ªÅ qu√° kh·ª©: {job['date_since']} -> {job['date_until']}")
        except Exception as e_fix:
            print(f"‚ö†Ô∏è L·ªói khi t√≠nh l·∫°i ng√†y Retry: {e_fix}")
    # ================= [K·∫æT TH√öC ƒêO·∫†N C·∫¶N TH√äM] =================

    # 2. X·ª≠ l√Ω qu√° h·∫°n Retry
    if retry_count >= MAX_RETRIES:
        print(f"üíÄ Job #{job_id} MAX RETRIES. Failed.")
        update_job_status(job_id, 'failed', f"Timeout: {retry_count} retries.")
        # C·∫≠p nh·∫≠t history l·∫ßn cu·ªëi n·∫øu c√≥
        conn = get_db()
        cur = conn.cursor()
        cur.execute("UPDATE job_history SET status='Failed', end_time=NOW() WHERE app_id=%s AND status IN ('Running','Processing')", (app_id,))
        conn.commit()
        conn.close()
        return 

    set_system_busy(True, app_id, 'auto')

    # 3. --- [QUAN TR·ªåNG] T√åM HO·∫∂C T·∫†O HISTORY ---
    # M·ª•c ƒë√≠ch: ƒê·ªÉ c√°c l·∫ßn Retry sau v·∫´n n·ªëi v√†o log c·ªßa l·∫ßn ƒë·∫ßu ti√™n
    hist_id = None
    try:
        conn = get_db()
        cur = conn.cursor()
        # T√¨m history ƒëang ch·∫°y d·ªü (Processing) c·ªßa App n√†y
        cur.execute("""
            SELECT id FROM job_history 
            WHERE app_id = %s AND status IN ('Running', 'Processing') 
            ORDER BY start_time DESC LIMIT 1
        """, (app_id,))
        row = cur.fetchone()
        
        if row:
            hist_id = row[0] # D√πng l·∫°i ID c≈© ƒë·ªÉ n·ªëi log
        else:
            # T·∫°o m·ªõi n·∫øu ch∆∞a c√≥ (L·∫ßn ch·∫°y ƒë·∫ßu ti√™n)
            cur.execute("""
                INSERT INTO job_history (app_id, start_time, status, run_type, logs, total_events)
                VALUES (%s, NOW(), 'Processing', 'schedule', '', 0)
                RETURNING id
            """, (app_id,))
            hist_id = cur.fetchone()[0]
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"‚ùå History Init Error: {e}")

    try:
        # H√†m log c·ª•c b·ªô: v·ª´a in ra m√†n h√¨nh, v·ª´a ƒë·∫©y v√†o DB ngay l·∫≠p t·ª©c
        def log(msg):
            print(msg)
            append_log_to_db(hist_id, msg)

        log(f"‚ñ∂Ô∏è Worker picking up Job #{job_id} (Retry: {retry_count}/{MAX_RETRIES})")

        try:
            # 1. Parse chu·ªói gi·ªù UTC t·ª´ Database ra
            utc_start = datetime.strptime(str(job['date_since']), '%Y-%m-%d %H:%M:%S')
            utc_end = datetime.strptime(str(job['date_until']), '%Y-%m-%d %H:%M:%S')
            
            # 2. C·ªông th√™m 7 ti·∫øng ƒë·ªÉ ra gi·ªù Vi·ªát Nam
            vn_start = utc_start + timedelta(hours=7)
            vn_end = utc_end + timedelta(hours=7)
            
            # 3. Format l·∫°i cho ƒë·∫πp (Gi·ªëng Terminal)
            log(f" üïí Scanning Window: VN[{vn_start.strftime('%H:%M')} - {vn_end.strftime('%H:%M')}] (UTC: {utc_start.strftime('%H:%M')} - {utc_end.strftime('%H:%M')})")
        except:
            # Fallback: N·∫øu l·ªói format th√¨ in nguy√™n g·ªëc
            log(f" üïí Scanning Window: {job['date_since']} -> {job['date_until']}")

        # L·∫•y th√¥ng tin App
        conn = get_db()
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT * FROM apps WHERE id = %s", (app_id,))
        app_info = cur.fetchone()
        cur.close()
        conn.close()

        if not app_info:
            log("‚ùå Error: App not found or deleted.")
            update_job_status(job_id, 'failed', 'App deleted')
            return

        # G·ªçi API AppMetrica
        url = "https://api.appmetrica.yandex.com/logs/v1/export/events.json"
        params = {
            "application_id": app_info['app_id'],
            "date_since": str(job['date_since']), 
            "date_until": str(job['date_until']),
            "fields": "event_name,event_timestamp,event_json",
            "limit": 1000000 
        }
        headers = {"Authorization": f"OAuth {app_info['api_token']}"}

        log(f"  üì° Connecting to AppMetrica...")
        response = requests.get(url, params=params, headers=headers, stream=True, timeout=600)
        
        if response.status_code == 200:
            log("  ‚úÖ Connection Established (200 OK). Downloading...")
            
            conn = get_db()
            cur = conn.cursor()
            
            data = response.json()
            events = data.get('data', [])
            event_count = len(events)
            
            # Insert d·ªØ li·ªáu
            for event in events:
                evt_name = event.get('event_name', 'unknown')
                evt_json = json.dumps(event)
                try: ts = datetime.fromtimestamp(int(event.get('event_timestamp')))
                except: ts = datetime.now()
                
                cur.execute("""
                    INSERT INTO event_logs (app_id, event_name, event_json, count, created_at) 
                    VALUES (%s, %s, %s, 1, %s)
                """, (app_id, evt_name, evt_json, ts))
            try:
                transform_events_to_level_analytics(app_id, events)
                # L·ªñI C≈®: logs(...) -> ƒê·ªîI TH√ÄNH log(...)
                log(f"ETL transform completed for {len(events)} events") 
            except Exception as e:
                # L·ªñI C≈®: logs(...) -> ƒê·ªîI TH√ÄNH log(...)
                log(f"ETL transform error: {str(e)}")       
            # --- C·∫¨P NH·∫¨T TR·∫†NG TH√ÅI CU·ªêI C√ôNG ---
            # Status: Success, c·∫≠p nh·∫≠t End Time -> Duration s·∫Ω t√≠nh ƒë√∫ng
            if hist_id:
                cur.execute("""
                    UPDATE job_history 
                    SET end_time = NOW(), status = 'Success', total_events = %s, success_count = %s
                    WHERE id = %s
                """, (event_count, event_count, hist_id))
            
            conn.commit()
            conn.close()
            
            update_job_status(job_id, 'completed', f"Done. {event_count} events.")
            log(f"  üéâ Job Completed. Imported: {event_count} events.")

        elif response.status_code == 202:
            wait_time=180
            log(f"  ‚è≥ HTTP 202: Data not ready. Waiting...")
            # Kh√¥ng ƒë√≥ng History, ƒë·ªÉ tr·∫°ng th√°i Processing ƒë·ªÉ l·∫ßn sau n·ªëi log ti·∫øp
            update_job_status(job_id, 'processing', 'Waiting for AppMetrica (202)...', inc_retry=True)
            time.sleep(wait_time) 
        
        elif response.status_code == 429:
            # 1. In to√†n b·ªô n·ªôi dung l·ªói ra ƒë·ªÉ xem n√≥ b·∫Øt ch·ªù bao l√¢u (th∆∞·ªùng n√≥ vi·∫øt trong n√†y)
            error_body = response.text
            log(f" ‚õî B·ªä CH·∫∂N (429)! N·ªôi dung t·ª´ Server: {error_body}")
            
            # 2. ƒê√°nh d·∫•u Job l√† FAILED ngay l·∫≠p t·ª©c (ƒê·ªÉ Worker kh√¥ng b·ªã k·∫πt)
            update_job_status(job_id, 'failed', f"Rate Limit 429. Server said: {error_body[:100]}...")
            
            # 3. ƒê√≥ng d√≤ng l·ªãch s·ª≠ ch·∫°y
            conn = get_db()
            cur = conn.cursor()
            cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
            conn.commit()
            conn.close()
            
            # 4. QUAN TR·ªåNG: Return lu√¥n ƒë·ªÉ tho√°t kh·ªèi h√†m, gi·∫£i ph√≥ng Worker
            log(" üõë D·ª´ng Job hi·ªán t·∫°i ƒë·ªÉ b·∫£o to√†n l·ª±c l∆∞·ª£ng. Vui l√≤ng ki·ªÉm tra log v√† th·ª≠ l·∫°i sau.")
            return

        else:
            log(f"  ‚ùå HTTP Error {response.status_code}")
            update_job_status(job_id, 'failed', f"HTTP {response.status_code}")
            # ƒê√≥ng History v√¨ l·ªói
            conn = get_db()
            cur = conn.cursor()
            cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
            conn.commit()
            conn.close()

    except Exception as e:
        log(f"  ‚ùå Exception: {str(e)}")
        update_job_status(job_id, 'failed', str(e))
        conn = get_db()
        cur = conn.cursor()
        cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
        conn.commit()
        conn.close()
    
    finally:
        set_system_busy(False)

def run_worker_loop():
    print("üöÄ Worker Loop Started...")
    while True:
        try:
            worker_process_jobs()
        except Exception as e:
            print(f"‚ùå Worker Loop Error: {e}")
        time.sleep(20)

# ==========================================
# PH·∫¶N 3: SCHEDULER TH√îNG MINH 
# ==========================================
def run_scheduler_loop():
    print("üöÄ Smart Scheduler Started (Anchor Time & Skip Logic)...")
    while True:
        try:
            now = datetime.now()
            conn = get_db()
            cur = conn.cursor(cursor_factory=RealDictCursor)
            cur.execute("SELECT * FROM apps WHERE is_active = true") 
            apps = cur.fetchall()

            for app in apps:
                app_id = app['id']
                interval_minutes = app.get('interval_minutes', 60) 
                if not interval_minutes: interval_minutes = 60

                sch_time_str = app.get('schedule_time', '00:00')
                try:
                    h, m = map(int, sch_time_str.split(':'))
                    anchor_time = now.replace(hour=h, minute=m, second=0, microsecond=0)
                except:
                    anchor_time = now.replace(hour=0, minute=0, second=0, microsecond=0)
                
                if anchor_time > now:
                    anchor_time = anchor_time - timedelta(days=1)
                
                diff = now - anchor_time
                cycles_passed = int(diff.total_seconds() // (interval_minutes * 60))
                
                expected_run_time = anchor_time + timedelta(minutes=cycles_passed * interval_minutes)
                
                time_since_expected = (now - expected_run_time).total_seconds()
                is_time_to_run = 0 <= time_since_expected < 65

                if is_time_to_run:
                    cur.execute("SELECT count(*) as count FROM etl_jobs WHERE app_id = %s AND created_at > %s", 
                                (app_id, now - timedelta(minutes=2)))
                    
                    if cur.fetchone()['count'] == 0:
                        print(f"‚è∞ Triggering Schedule for App #{app_id} at {now.strftime('%H:%M:%S')}")
                        
                        if is_system_busy():
                            print(f"‚ö†Ô∏è SKIPPING Auto Schedule for App #{app_id} - System is BUSY")
                            cur.execute("""
                                INSERT INTO job_history (app_id, start_time, status, run_type, logs) 
                                VALUES (%s, NOW(), 'Skipped', 'schedule', 'Skipped due to System Busy (Conflict)')
                            """, (app_id,))
                            conn.commit()
                        else:
                            delay_minutes = 90
                            end_time_vn = now - timedelta(minutes=delay_minutes)
                            start_time_vn = end_time_vn - timedelta(minutes=interval_minutes)
                            
                            end_time_utc = end_time_vn - timedelta(hours=7)
                            start_time_utc = start_time_vn - timedelta(hours=7)
                            
                            date_until = end_time_utc.strftime('%Y-%m-%d %H:%M:%S')
                            date_since = start_time_utc.strftime('%Y-%m-%d %H:%M:%S')

                            print(f"  üé´ Creating Job: VN[{start_time_vn.strftime('%H:%M')} - {end_time_vn.strftime('%H:%M')}] -> UTC[{date_since} - {date_until}]")
                            create_etl_job(app_id, date_since, date_until)

            cur.close()
            conn.close()
        except Exception as e:
            print(f"‚ùå Scheduler Error: {e}")
        
        time.sleep(60)

# ==========================================
# PH·∫¶N 4: LOGIC CH·∫†Y TAY (MANUAL) - [FIXED RETRY LOGIC]
# ==========================================
def perform_manual_etl(app_id, run_type='manual', is_demo=False, retry_job_id=None):
    """
    H√†m x·ª≠ l√Ω ch·∫°y tay (Manual), Demo v√† RETRY CHU·∫®N X√ÅC.
    """
    if is_system_busy():
        print(f"‚ùå System BUSY. Skip run for App {app_id}.")
        return

    set_system_busy(True, app_id, run_type)
    hist_id = None

    try:
        conn = get_db()
        if not conn: return
        
        # 1. T·∫†O HISTORY RECORD
        msg_start = f"üöÄ Starting {run_type.upper()} run..."
        if retry_job_id: msg_start += f" (Retry of Job #{retry_job_id})"
        
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO job_history (app_id, start_time, status, run_type, logs, total_events) 
            VALUES (%s, NOW(), 'Running', %s, %s, 0) 
            RETURNING id
        """, (app_id, run_type, f"[{datetime.now().strftime('%H:%M:%S')}] {msg_start}"))
        hist_id = cur.fetchone()[0]
        conn.commit()
        
        # [M·ªöI] T·∫†O C·ªú D·ª™NG CHO JOB N√ÄY
        stop_event = threading.Event()
        JOB_STOP_EVENTS[hist_id] = stop_event

        # H√†m log helper
        def log(msg):
            print(msg)
            append_log_to_db(hist_id, msg)

        # 2. L·∫§Y C·∫§U H√åNH APP
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT * FROM apps WHERE id=%s", (app_id,))
        app = cur.fetchone()
        
        if not app:
            log("‚ùå Error: App ID not found.")
            return

        # 3. C·∫§U H√åNH TH·ªúI GIAN (LOGIC QUAN TR·ªåNG ƒê√É S·ª¨A)
        date_since = None
        date_until = None
        
        # --- CASE 1: RETRY (L·∫•y gi·ªù t·ª´ Job c≈©) ---
        if run_type == 'retry' and retry_job_id:
            cur.execute("SELECT start_time FROM job_history WHERE id = %s", (retry_job_id,))
            old_job = cur.fetchone()
            if old_job and old_job['start_time']:
                target_time = old_job['start_time']
                # T√≠nh l·∫°i c·ª≠a s·ªï theo ƒë√∫ng logic c≈© (th∆∞·ªùng l√† 1 ti·∫øng tr∆∞·ªõc ƒë√≥)
                # Gi·∫£ s·ª≠ chu k·ª≥ chu·∫©n l√† 60 ph√∫t + 5 ph√∫t d∆∞
                fix_to = target_time
                fix_from = target_time - timedelta(minutes=65)
                
                date_until = fix_to.strftime('%Y-%m-%d %H:%M:%S')
                date_since = fix_from.strftime('%Y-%m-%d %H:%M:%S')
                log(f"üîô RETRY MODE: L√πi th·ªùi gian v·ªÅ qu√° kh·ª© theo Job #{retry_job_id}")
            else:
                log(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Job c≈© #{retry_job_id}, chuy·ªÉn sang Manual Mode.")

        # --- CASE 2: DEMO / MANUAL (N·∫øu kh√¥ng ph·∫£i Retry ho·∫∑c Retry l·ªói) ---
        if not date_since:
            now = datetime.now()
            if run_type == 'demo':
                delay_minutes = 45; duration_minutes = 15
                log(f"üß™ DEMO MODE: Target Time = NOW - {delay_minutes}m")
            else:
                delay_minutes = 60; duration_minutes = 30
                log(f"‚öôÔ∏è MANUAL MODE: Target Time = NOW - {delay_minutes}m")

            end_dt = now - timedelta(minutes=delay_minutes)
            start_dt = end_dt - timedelta(minutes=duration_minutes)
            
            date_since = start_dt.strftime('%Y-%m-%d %H:%M:%S')
            date_until = end_dt.strftime('%Y-%m-%d %H:%M:%S')

        log(f" üïí Scanning Window: {date_since} -> {date_until}")
        
        # 4. G·ªåI API APPMETRICA
        url = "https://api.appmetrica.yandex.com/logs/v1/export/events.json"
        params = {
            "application_id": app['app_id'],
            "date_since": date_since,
            "date_until": date_until,
            "fields": "event_name,event_timestamp,event_json",
            "limit": 1000000
        }
        headers = {"Authorization": f"OAuth {app['api_token']}"}
        
        status = "Failed"
        total_events = 0
        max_retries = 18 # Gi·∫£m xu·ªëng 18 ƒë·ªÉ tr√°nh spam, v√¨ ta ƒë√£ tƒÉng th·ªùi gian ch·ªù
        
        for i in range(max_retries):
            # [M·ªöI] KI·ªÇM TRA C·ªú D·ª™NG ƒê·∫¶U V√íNG L·∫∂P
            if stop_event.is_set():
                log("üõë USER STOPPED PROCESS.")
                status = "Cancelled"
                break

            log(f"üì° Requesting AppMetrica (Attempt {i+1}/{max_retries})...")
            resp = requests.get(url, params=params, headers=headers)
            
            if resp.status_code == 200:
                data = resp.json().get('data', [])
                total_events = len(data)
                log(f"‚úÖ Success! Received {total_events} events. Importing...")
                try:
                    # T√°ch ri√™ng connection cho vi·ªác insert ƒë·ªÉ an to√†n
                    conn_insert = get_db()
                    cur_insert = conn_insert.cursor()
                    
                    # D√πng Batch Insert (Executemany) cho nhanh v√† √≠t l·ªói h∆°n insert t·ª´ng d√≤ng
                    values = []
                    for d in data:
                        evt_json = json.dumps(d)
                        try: ts = datetime.fromtimestamp(int(d.get('event_timestamp')))
                        except: ts = datetime.now()
                        
                        values.append((
                            app_id, 
                            d.get('event_name', 'unknown'), 
                            evt_json, 
                            1, 
                            ts
                        ))
                    
                    # Th·ª±c hi·ªán Insert 1 l·∫ßn
                    query = """
                        INSERT INTO event_logs (app_id, event_name, event_json, count, created_at) 
                        VALUES (%s, %s, %s, %s, %s)
                    """
                    cur_insert.executemany(query, values)
                    conn_insert.commit()
                    conn_insert.close()
                    
                    # QUAN TR·ªåNG: G√°n status Success sau khi insert th√†nh c√¥ng
                    status = "Success"
                    log(f"üéâ Done. Imported {total_events} events to DB.")
                    
                except Exception as e_insert:
                    log(f"‚ùå DB Insert Error: {str(e_insert)}")
                    status = "Failed" # ƒê√°nh d·∫•u fail n·∫øu l·ªói DB

                break
                
                # Insert v√†o DB
                conn_insert = get_db()
                cur_insert = conn_insert.cursor()
                for d in data:
                    evt_json = json.dumps(d)
                    try: ts = datetime.fromtimestamp(int(d.get('event_timestamp')))
                    except: ts = datetime.now()
                    cur_insert.execute("INSERT INTO event_logs (app_id, event_name, event_json, count, created_at) VALUES (%s, %s, %s, 1, %s)", 
                                (app_id, d.get('event_name'), evt_json, ts))
                conn_insert.commit()
                conn_insert.close()
                
                status = "Success"
                log(f"üéâ Done. Imported {total_events} events.")
                break
            
            elif resp.status_code == 202:
                # TƒÉng th·ªùi gian ch·ªù l√™n 3 ph√∫t
                log(f"‚è≥ Server 202 (Preparing Data). Waiting 180s...")
                if stop_event.wait(180): 
                    log("üõë Stop Signal received while waiting.")
                    status = "Cancelled"
                    break
            
            elif resp.status_code == 429:
                # X·ª≠ l√Ω 429 th√¥ng minh: D·ª´ng ngay l·∫≠p t·ª©c
                error_text = resp.text
                log(f"‚õî B·ªä CH·∫∂N (429)! Server message: {error_text}")
                status = "Failed"
                break # Tho√°t v√≤ng l·∫∑p ngay
            
            else:
                log(f"‚ùå Error {resp.status_code}: {resp.text}")
                status = "Failed"
                if stop_event.wait(30): break
        else:
            log("‚ùå Timeout: AppMetrica did not return data after max retries.")
            status = "Failed"
    
    except Exception as e:
        log(f"‚ùå Critical Error: {str(e)}")
        status = "Failed"
    
    finally:
        # D·ªçn d·∫πp c·ªù
        if hist_id and hist_id in JOB_STOP_EVENTS:
            del JOB_STOP_EVENTS[hist_id]
        set_system_busy(False)
        if hist_id:
            try:
                conn_end = get_db()
                cur_end = conn_end.cursor()
                cur_end.execute("""
                    UPDATE job_history 
                    SET end_time=NOW(), status=%s, total_events=%s 
                    WHERE id=%s
                """, (status, total_events, hist_id))
                conn_end.commit()
                conn_end.close()
            except: pass

# ==========================================
# PH·∫¶N 5: API ENDPOINTS (ƒê√É C·∫¨P NH·∫¨T DASHBOARD)
# ==========================================

@app.route("/monitor/history", methods=['GET'])
def get_history():
    app_id = request.args.get('app_id') 
    # [M·ªöI] L·∫•y tham s·ªë ph√¢n trang
    try:
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 30)) # M·∫∑c ƒë·ªãnh 30 d√≤ng/trang
    except:
        page = 1; limit = 30
        
    offset = (page - 1) * limit
    conn = get_db()
    if not conn: return jsonify([])
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # 1. X√¢y d·ª±ng m·ªánh ƒë·ªÅ WHERE
        where_clause = ""
        params_count = []
        if app_id: 
            where_clause = "WHERE h.app_id = %s"
            params_count.append(app_id)

        # 2. ƒê·∫øm t·ªïng s·ªë records (ƒë·ªÉ t√≠nh s·ªë trang)
        cur.execute(f"SELECT COUNT(*) as total FROM job_history h {where_clause}", tuple(params_count))
        total_records = cur.fetchone()['total']
        total_pages = (total_records + limit - 1) // limit

        # 3. L·∫•y d·ªØ li·ªáu ph√¢n trang
        query = f"""
            SELECT h.*, a.name as app_name 
            FROM job_history h 
            JOIN apps a ON h.app_id = a.id 
            {where_clause}
            ORDER BY h.start_time DESC 
            LIMIT %s OFFSET %s
        """
        # Copy params t·ª´ count sang v√† th√™m limit/offset
        params_data = params_count + [limit, offset]
        
        cur.execute(query, tuple(params_data))
        res = cur.fetchall()

        # --- [FIX L·ªñI TIMEZONE V√Ä DURATION] ---
        # T√≠nh to√°n Duration v√† Format l·∫°i Time ƒë·ªÉ tr√°nh Frontend t·ª± c·ªông +7 ti·∫øng
        for row in res:
            # 1. T√≠nh Duration
            duration_str = "..."
            if row['start_time'] and row['end_time']:
                diff = row['end_time'] - row['start_time']
                total_seconds = int(diff.total_seconds())
                minutes = total_seconds // 60
                seconds = total_seconds % 60
                duration_str = f"{minutes} min {seconds} sec"
            elif row['status'] == 'Running':
                 duration_str = "Running..."
            
            row['duration'] = duration_str

            # 2. Fix Timezone: Chuy·ªÉn datetime th√†nh string c·ª©ng
            # ƒê·ªÉ Frontend hi·ªÉn th·ªã y nguy√™n gi·ªù c·ªßa Server (Local VN)
            if row['start_time']:
                row['start_time'] = row['start_time'].strftime('%d/%m/%Y %H:%M:%S')
            if row['end_time']:
                row['end_time'] = row['end_time'].strftime('%d/%m/%Y %H:%M:%S')

        return jsonify({
            "data": res,
            "pagination": {
                "current_page": page,
                "total_pages": total_pages,
                "total_records": total_records
            }
        })
    except Exception as e:
        print(f"History Error: {e}")
        return jsonify({"data": [], "pagination": {}})    
    finally: conn.close()

@app.route("/monitor/purge", methods=['DELETE'])
def purge_history():
    conn = get_db()
    try:
        cur = conn.cursor()
        cur.execute("DELETE FROM job_history")
        conn.commit()
        return jsonify({"msg": "History Cleared"})
    finally: conn.close()

@app.route("/apps", methods=['GET', 'POST'])
def handle_apps():
    conn = get_db()
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        if request.method == 'GET':
            cur.execute("SELECT * FROM apps ORDER BY id ASC")
            return jsonify(cur.fetchall())
        else:
            d = request.json
            cur.execute("INSERT INTO apps (name, app_id, api_token, is_active, schedule_time, interval_minutes) VALUES (%s, %s, %s, %s, %s, %s)", 
                        (d['name'], d['app_id'], d['api_token'], d['is_active'], d.get('schedule_time', '12:00'), d.get('interval_minutes', 60)))
            conn.commit()
            return jsonify({"msg": "Created"})
    finally: conn.close()

@app.route("/apps/<int:id>", methods=['PUT', 'DELETE'])
def update_app(id):
    conn = get_db()
    try:
        cur = conn.cursor()
        if request.method == 'PUT':
            d = request.json
            cur.execute("UPDATE apps SET name=%s, app_id=%s, api_token=%s, is_active=%s, schedule_time=%s, interval_minutes=%s WHERE id=%s", 
                        (d['name'], d['app_id'], d['api_token'], d['is_active'], d.get('schedule_time', '12:00'), d.get('interval_minutes', 60), id))
            conn.commit()
            return jsonify({"msg": "Updated"})
        elif request.method == 'DELETE':
            cur.execute("DELETE FROM event_logs WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM job_history WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM etl_jobs WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM apps WHERE id=%s", (id,))
            conn.commit()
            return jsonify({"msg": "Deleted"})
    finally: conn.close()

@app.route("/etl/run/<int:app_id>", methods=['POST'])
def run_etl_api(app_id):
    # --- CODE M·ªöI ---
    data = request.json
    run_type = data.get('run_type', 'manual') # L·∫•y lo·∫°i ch·∫°y (manual/retry/demo)
    retry_job_id = data.get('retry_job_id')   # L·∫•y ID c·ªßa job c≈© n·∫øu l√† retry
    
    is_demo = (run_type == 'demo')
    
    if is_system_busy():
         return jsonify({"status": "error", "message": "System is busy processing another job. Please skip this cycle."}), 409

    # Truy·ªÅn th√™m retry_job_id v√†o h√†m x·ª≠ l√Ω
    threading.Thread(target=perform_manual_etl, args=(app_id, run_type, is_demo, retry_job_id)).start()
    return jsonify({"status": "started", "mode": run_type})

# --- [FIXED] √âP KI·ªÇU TEXT -> JSONB ƒê·ªÇ TR√ÅNH L·ªñI OPERATOR ---
@app.route("/dashboard/<int:app_id>", methods=['GET'])
def get_dashboard(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB Connection failed"})
    
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)

       # Chu·∫©n b·ªã ƒëi·ªÅu ki·ªán l·ªçc ng√†y (D√πng chung)
        time_condition = ""
        params = [app_id]
        if start_date:
            time_condition += " AND created_at >= %s"
            params.append(start_date + " 00:00:00")
        if end_date:
            time_condition += " AND created_at <= %s"
            params.append(end_date + " 23:59:59")

        # --- QUERY 1: T·ªîNG EVENT (ƒê·∫øm tr·ª±c ti·∫øp, si√™u nhanh nh·ªù Index) ---
        cur.execute(f"""
            SELECT COUNT(*) as total FROM event_logs 
            WHERE app_id = %s {time_condition}
        """, tuple(params))
        total_events = cur.fetchone()['total']
        
        # --- QUERY 2: DOANH THU BOOSTER (Join tr·ª±c ti·∫øp, KH√îNG parse JSON) ---
        # Ch·ªâ join d·ª±a tr√™n event_name -> T·∫≠n d·ª•ng Index -> Nhanh g·∫•p 10 l·∫ßn
        cur.execute(f"""
            SELECT 
                b.display_name as name,
                COUNT(*)::int as usage_count,
                COALESCE(SUM(b.cost), 0)::int as total_spent
            FROM event_logs e
            JOIN booster_configs b ON e.event_name = b.booster_event_name
            WHERE e.app_id = %s {time_condition}
            GROUP BY b.display_name
            ORDER BY total_spent DESC
            LIMIT 5
        """, tuple(params))
        booster_stats = cur.fetchall()
        total_revenue = sum(item['total_spent'] for item in booster_stats) if booster_stats else 0

        # --- QUERY 3: TOP EVENTS (Group by c·ªôt c√≥ s·∫µn) ---
        cur.execute(f"""
            SELECT event_name as name, COUNT(*)::int as value
            FROM event_logs
            WHERE app_id = %s {time_condition}
            GROUP BY event_name
            ORDER BY value DESC
            LIMIT 5
        """, tuple(params))
        chart_data = cur.fetchall()

        # --- QUERY 4: ACTIVE USERS (Ph·∫ßn n·∫∑ng nh·∫•t - ƒê·ªÉ cu·ªëi c√πng) ---
        # Ch·ªâ parse JSON cho vi·ªác ƒë·∫øm user, kh√¥ng ·∫£nh h∆∞·ªüng c√°c ch·ªâ s·ªë kia
        cur.execute(f"""
            SELECT COUNT(DISTINCT 
                COALESCE(
                    -- Ki·ªÉm tra xem trong JSON c√≥ key t√™n l√† "event_json" kh√¥ng?
                    -- D√πng to√°n t·ª≠ ? c·ªßa Postgres (An to√†n tuy·ªát ƒë·ªëi)
                    CASE WHEN event_json::jsonb ? 'event_json' 
                         THEN (event_json::jsonb->>'event_json')::jsonb->>'userID'
                         ELSE event_json::jsonb->>'userID'
                    END,
                    'Guest'
                )
            ) as active 
            FROM event_logs 
            WHERE app_id = %s {time_condition}
        """, tuple(params))
        active_users = cur.fetchone()['active']

        return jsonify({
            "success": True,
            "overview": {
                "cards": {
                    "revenue": total_revenue,
                    "active_users": active_users,
                    "total_events": total_events
                },
                "chart_main": chart_data,
                "booster_chart": booster_stats
            }
        })

    except Exception as e:
        print(f"Error dashboard: {e}")
        return jsonify({
            "success": False, 
            "error": str(e)
        }), 500
    finally:
        conn.close()

# --- API M·ªöI: L·∫§Y CHI TI·∫æT LEVEL (CHO TAB 2) - [UPDATED: DATE FILTER] ---
@app.route("/dashboard/<int:app_id>/level-detail", methods=['GET'])
def get_level_detail(app_id):
    # 1. L·∫•y tham s·ªë Level & Date t·ª´ Frontend
    level_id = request.args.get('level_id')
    start_date = request.args.get('start_date') # Format: YYYY-MM-DD
    end_date = request.args.get('end_date')     # Format: YYYY-MM-DD
    
    if not level_id:
        return jsonify({"success": False, "error": "Thi·∫øu tham s·ªë level_id"}), 400

    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB Error"}), 500
    
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # 2. X√ÇY D·ª∞NG C√ÇU ƒêI·ªÄU KI·ªÜN ƒê·ªòNG (DYNAMIC WHERE)
        # M·∫∑c ƒë·ªãnh l·ªçc theo App v√† Level
        where_clause = "WHERE app_id = %s AND level_id = %s"
        params = [app_id, str(level_id)]
        
        # N·∫øu c√≥ ng√†y b·∫Øt ƒë·∫ßu -> Th√™m ƒëi·ªÅu ki·ªán >= 00:00:00
        if start_date:
            where_clause += " AND event_time >= %s"
            params.append(start_date + " 00:00:00")
            
        # N·∫øu c√≥ ng√†y k·∫øt th√∫c -> Th√™m ƒëi·ªÅu ki·ªán <= 23:59:59
        if end_date:
            where_clause += " AND event_time <= %s"
            params.append(end_date + " 23:59:59")

        # --- QUERY 1: METRICS CARDS (C1) ---
        # L∆∞u √Ω: D√πng f-string ƒë·ªÉ ch√®n where_clause, c√≤n params truy·ªÅn qua tuple ƒë·ªÉ b·∫£o m·∫≠t
        cur.execute(f"""
            SELECT 
                COUNT(*) FILTER (WHERE event_type = 'START') as total_plays,
                ROUND(
                    (COUNT(*) FILTER (WHERE event_type = 'WIN')::numeric / 
                    NULLIF(COUNT(*) FILTER (WHERE event_type IN ('WIN', 'FAIL')), 0)) * 100, 
                1)::float as win_rate,
                ROUND(SUM(coin_spent)::numeric / NULLIF(COUNT(DISTINCT user_id), 0), 0)::int as arpu
            FROM view_game_stats_cleaned
            {where_clause}
        """, tuple(params))
        metrics = cur.fetchone()

        # --- QUERY 2: TOP ITEM ---
        cur.execute(f"""
            SELECT item_name, COUNT(*) as quantity 
            FROM view_game_stats_cleaned 
            {where_clause} AND event_type = 'SPEND'
            GROUP BY item_name 
            ORDER BY quantity DESC 
            LIMIT 1
        """, tuple(params))
        top_item = cur.fetchone()
        metrics['top_item'] = top_item['item_name'] if top_item else "Kh√¥ng c√≥"

        # --- QUERY 3: PH·ªÑU CH·∫æT (FUNNEL C2) ---
        cur.execute(f"""
            SELECT event_type, COUNT(*) as count, SUM(coin_spent) as revenue
            FROM view_game_stats_cleaned
            {where_clause}
            GROUP BY event_type
            ORDER BY count DESC
        """, tuple(params))
        funnel_data = cur.fetchall()

        # --- QUERY 4: LOGS (D) ---
        cur.execute(f"""
            SELECT 
                to_char(event_time, 'HH24:MI:SS DD/MM') as time,
                user_id,
                event_name,
                item_name,
                coin_spent
            FROM view_game_stats_cleaned
            {where_clause}
            ORDER BY event_time DESC
            LIMIT 100
        """, tuple(params))
        logs_data = cur.fetchall()

        return jsonify({
            "success": True,
            "level_id": level_id,
            "filter": {"start": start_date, "end": end_date}, # Tr·∫£ v·ªÅ ƒë·ªÉ Frontend bi·∫øt ƒëang l·ªçc g√¨
            "metrics": metrics, 
            "funnel": funnel_data, 
            "logs": logs_data      
        })

    except Exception as e:
        print(f"‚ùå Level Detail Error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally:
        conn.close()

# --- B·ªî SUNG API: X√ìA 1 D√íNG & STOP JOB ---
@app.route("/monitor/history/<int:id>", methods=['DELETE'])
def delete_single_history(id):
    conn = get_db()
    try:
        cur = conn.cursor()
        cur.execute("DELETE FROM job_history WHERE id = %s", (id,))
        conn.commit()
        return jsonify({"success": True, "msg": f"Deleted history #{id}"})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

@app.route("/etl/stop/<int:hist_id>", methods=['POST'])
def stop_etl_process(hist_id):
    # API n√†y d√πng ƒë·ªÉ ƒë√°nh d·∫•u job l√† Cancelled tr√™n Database
    # N√≥ c≈©ng c·ªë g·∫Øng reset tr·∫°ng th√°i b·∫≠n c·ªßa h·ªá th·ªëng n·∫øu c·∫ßn
    conn = get_db()
    try:
        # 1. K√≠ch ho·∫°t c·ªù d·ª´ng ƒë·ªÉ Worker ƒëang ch·∫°y t·ª± tho√°t
        if hist_id in JOB_STOP_EVENTS:
            print(f"üõë Sending STOP signal to Job #{hist_id}...")
            JOB_STOP_EVENTS[hist_id].set() # ƒê√°nh th·ª©c Worker ngay l·∫≠p t·ª©c
        cur = conn.cursor()
        # C·∫≠p nh·∫≠t DB
        cur.execute("""
            UPDATE job_history 
            SET status = 'Cancelled', end_time = NOW(), logs = logs || E'\n[USER MANUAL STOP]'
            WHERE id = %s AND status IN ('Running', 'Processing')
        """, (hist_id,))
        conn.commit()

        # 3. Reset h·ªá th·ªëng n·∫øu c·∫ßn
        if is_system_busy():
            set_system_busy(False)
            
        return jsonify({"success": True, "msg": f"Stop signal sent to Job #{hist_id}"})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

# --- [FIXED] S·ª¨A T√äN C·ªòT CONFIG CHO KH·ªöP DB ---
@app.route("/apps/<int:app_id>/analytics-config", methods=['GET', 'POST'])
def handle_analytics_config(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB Connection Failed"}), 500
    
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        if request.method == 'GET':
            cur.execute("""
                SELECT level_start_event as level_start, 
                       level_win_event as level_win, 
                       level_fail_event as level_fail
                FROM analytics_config WHERE app_id = %s
            """, (app_id,))
            event_row = cur.fetchone()
            
            # [FIX QUAN TR·ªåNG] D√πng booster_event_name v√† cost
            cur.execute("""
                SELECT booster_event_name as event_name, display_name, cost as coin_cost
                FROM booster_configs WHERE app_id = %s
            """, (app_id,))
            booster_rows = cur.fetchall()
            
            response_data = {
                "events": event_row if event_row else {"level_start": "", "level_win": "", "level_fail": ""},
                "boosters": booster_rows if booster_rows else []
            }
            return jsonify(response_data)

        elif request.method == 'POST':
            payload = request.json
            events = payload.get('events', {})
            boosters = payload.get('boosters', [])
            
            # Upsert analytics_config (Gi·ªØ nguy√™n)
            cur.execute("""
                INSERT INTO analytics_config (app_id, level_start_event, level_win_event, level_fail_event, updated_at)
                VALUES (%s, %s, %s, %s, NOW())
                ON CONFLICT (app_id) DO UPDATE SET 
                    level_start_event = EXCLUDED.level_start_event,
                    level_win_event = EXCLUDED.level_win_event,
                    level_fail_event = EXCLUDED.level_fail_event,
                    updated_at = NOW()
            """, (app_id, events.get('level_start', ''), events.get('level_win', ''), events.get('level_fail', '')))
            
            # Update booster_configs (X√≥a ƒëi insert l·∫°i)
            cur.execute("DELETE FROM booster_configs WHERE app_id = %s", (app_id,))
            
            if boosters:
                values = []
                for b in boosters:
                    values.append((
                        app_id, 
                        b.get('event_name', ''), # Frontend g·ª≠i l√™n l√† event_name
                        b.get('display_name', ''), 
                        int(b.get('coin_cost', 0)) # Frontend g·ª≠i l√™n l√† coin_cost
                    ))
                
                # [FIX QUAN TR·ªåNG] Insert v√†o ƒë√∫ng c·ªôt booster_event_name v√† cost
                query = """
                    INSERT INTO booster_configs (app_id, booster_event_name, display_name, cost, created_at)
                    VALUES (%s, %s, %s, %s, NOW())
                """
                cur_batch = conn.cursor()
                cur_batch.executemany(query, values)
                cur_batch.close()
            
            conn.commit()
            return jsonify({"success": True, "msg": "Configuration Saved Successfully"})
            
    except Exception as e:
        print(f"‚ùå Analytics Config Error: {e}")
        conn.rollback()
        return jsonify({"success": False, "error": str(e)}), 500
    finally:
        conn.close()

# --- API CH·∫†Y ETL (T·ªîNG H·ª¢P D·ªÆ LI·ªÜU) ---
@app.route("/api/run-etl/<int:app_id>", methods=['POST'])
def trigger_etl_process(app_id):
    # Ch·∫°y trong thread ri√™ng ƒë·ªÉ kh√¥ng block server
    threading.Thread(target=run_etl_pipeline, args=(app_id,)).start()
    return jsonify({"status": "started", "message": "ETL process started in background"})

# --- API M·ªöI: TRA C·ª®U D·ªÆ LI·ªÜU TH√î (DATA EXPLORER) ---
@app.route("/events/search", methods=['GET'])
def search_events():
    try:
        app_id = request.args.get('app_id')
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 20))
        
        # C√°c b·ªô l·ªçc
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        event_name = request.args.get('event_name')
        keyword = request.args.get('keyword') # T√¨m UserID ho·∫∑c n·ªôi dung b·∫•t k·ª≥ trong JSON

        if not app_id:
            return jsonify({"success": False, "error": "Missing app_id"}), 400

        conn = get_db()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # 1. X√¢y d·ª±ng c√¢u WHERE ƒë·ªông
        where_clauses = ["app_id = %s"]
        params = [app_id]

        if start_date:
            where_clauses.append("created_at >= %s")
            params.append(start_date + " 00:00:00")
        
        if end_date:
            where_clauses.append("created_at <= %s")
            params.append(end_date + " 23:59:59")

        if event_name and event_name.strip():
            where_clauses.append("event_name = %s")
            params.append(event_name)

        if keyword and keyword.strip():
            # K·ªπ thu·∫≠t t√¨m ki·∫øm trong JSON (Chuy·ªÉn JSON th√†nh Text ƒë·ªÉ t√¨m)
            where_clauses.append("event_json::text ILIKE %s")
            params.append(f"%{keyword}%")

        full_where = " WHERE " + " AND ".join(where_clauses)

        # 2. ƒê·∫øm t·ªïng s·ªë d√≤ng (ƒë·ªÉ l√†m ph√¢n trang 1/100...)
        count_query = f"SELECT COUNT(*) as total FROM event_logs {full_where}"
        cursor.execute(count_query, tuple(params))
        total_records = cursor.fetchone()['total']
        total_pages = (total_records + limit - 1) // limit

        # 3. L·∫•y d·ªØ li·ªáu ph√¢n trang
        offset = (page - 1) * limit
        data_query = f"""
            SELECT 
                id, 
                event_name, 
                to_char(created_at, 'YYYY-MM-DD HH24:MI:SS') as created_at,
                event_json -- L·∫•y nguy√™n c·ª•c JSON v·ªÅ ƒë·ªÉ Frontend hi·ªÉn th·ªã ƒë·∫πp
            FROM event_logs 
            {full_where}
            ORDER BY created_at DESC
            LIMIT %s OFFSET %s
        """
        # Th√™m limit/offset v√†o params
        params.extend([limit, offset])
        
        cursor.execute(data_query, tuple(params))
        rows = cursor.fetchall()

        # 4. Tr√≠ch xu·∫•t s∆° b·ªô User ID ƒë·ªÉ hi·ªÉn th·ªã ra ngo√†i b·∫£ng (cho ti·ªán nh√¨n)
        for row in rows:
            try:
                # Parse JSON string th√†nh Dict
                import json
                raw = row['event_json']
                # X·ª≠ l√Ω double-encoded n·∫øu c√≥
                if isinstance(raw, str):
                    parsed = json.loads(raw)
                    # N·∫øu b√™n trong l·∫°i c√≥ key 'event_json' d·∫°ng string
                    if isinstance(parsed, dict) and 'event_json' in parsed and isinstance(parsed['event_json'], str):
                        inner = json.loads(parsed['event_json'])
                        parsed.update(inner)
                    row['event_json'] = parsed # G√°n l·∫°i object ƒë√£ s·∫°ch
                
                # --- [LOGIC M·ªöI] T·∫†O C·ªòT KEY INFO ---
                # Thay v√¨ l·∫•y UserID, ta l·∫•y th√¥ng tin ng·ªØ c·∫£nh quan tr·ªçng h∆°n
                data = row['event_json']
                info_parts = []
                
                # 1. N·∫øu c√≥ th√¥ng tin Level/Mission -> L·∫•y ngay
                if 'levelID' in data: info_parts.append(f"Lv.{data['levelID']}")
                if 'missionID' in data: info_parts.append(f"Ms.{data['missionID']}")
                
                # 2. N·∫øu c√≥ th√¥ng tin Ti·ªÅn/Gi√° -> L·∫•y ngay
                if 'coin_cost' in data: info_parts.append(f"-{data['coin_cost']} Coin")
                if 'coin_price' in data: info_parts.append(f"-{data['coin_price']} Coin")
                if 'revenue' in data: info_parts.append(f"+{data['revenue']} USD")
                
                # 3. N·∫øu c√≥ th√¥ng tin Item/Booster
                if 'item_name' in data: info_parts.append(data['item_name'])
                
                # G√°n v√†o bi·∫øn m·ªõi ƒë·ªÉ tr·∫£ v·ªÅ Frontend
                row['key_info'] = " | ".join(info_parts) if info_parts else "..."
            except:
                row['key_info'] = '-'
                row['event_json'] = {}

        return jsonify({
            "success": True,
            "data": rows,
            "pagination": {
                "current_page": page,
                "total_pages": total_pages,
                "total_records": total_records,
                "limit": limit
            }
        })

    except Exception as e:
        print(f"Search Error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally:
        conn.close()

if __name__ == '__main__':
    t1 = threading.Thread(target=run_scheduler_loop)
    t1.daemon = True
    t1.start()

    t2 = threading.Thread(target=run_worker_loop)
    t2.daemon = True
    t2.start()

    print("üöÄ SYSTEM READY: Smart Scheduler & Worker Threads started...")
    app.run(host='0.0.0.0', port=8080, debug=True, use_reloader=False)