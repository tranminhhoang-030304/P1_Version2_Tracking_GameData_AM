from etl_processor import run_etl_pipeline
from flask import Flask, jsonify, request
from flask_cors import CORS
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime, timedelta
import time
import requests
import threading
import random
import os                       
from dotenv import load_dotenv
import re

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

load_dotenv()
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# --- 3. S·ª¨A C·∫§U H√åNH DATABASE (L·∫•y t·ª´ .env) ---
DB_HOST = os.getenv("DB_HOST")
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASS = os.getenv("DB_PASS") 

# Ki·ªÉm tra an to√†n: N·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c pass th√¨ b√°o l·ªói
if not DB_PASS:
    print("‚ö†Ô∏è Ch∆∞a t√¨m th·∫•y DB_PASS trong file .env")

# --- QU·∫¢N L√ù TR·∫†NG TH√ÅI ƒêA LU·ªíNG (PARALLEL MODE) ---
# Thay v√¨ kh√≥a c·∫£ h·ªá th·ªëng, ch·ªâ kh√≥a t·ª´ng App ID ƒëang ch·∫°y
RUNNING_APPS = set()
APP_LOCK = threading.Lock()
JOB_STOP_EVENTS = {}

def is_app_busy(app_id):
    with APP_LOCK:
        return app_id in RUNNING_APPS

def try_lock_app(app_id):
    """
    C·ªë g·∫Øng kh√≥a App ƒë·ªÉ ch·∫°y Job. 
    Tr·∫£ v·ªÅ True n·∫øu kh√≥a th√†nh c√¥ng (App ƒëang r·∫£nh).
    Tr·∫£ v·ªÅ False n·∫øu App ƒëang b·∫≠n ch·∫°y job kh√°c.
    """
    with APP_LOCK:
        if app_id in RUNNING_APPS:
            return False
        RUNNING_APPS.add(app_id)
        return True

def unlock_app(app_id):
    """Gi·∫£i ph√≥ng App sau khi ch·∫°y xong"""
    with APP_LOCK:
        RUNNING_APPS.discard(app_id)

# Gi·ªØ h√†m n√†y ƒë·ªÉ t∆∞∆°ng th√≠ch code c≈©, nh∆∞ng logic tr·∫£ v·ªÅ False lu√¥n ƒë·ªÉ kh√¥ng ch·∫∑n global
def is_system_busy(): return False

def set_system_busy(busy, app_id=None, run_type=None): pass

def universal_flatten(raw_input):
    if not raw_input: return {}
    
    data = {}
    # L·ªõp 1: Parse t·ª´ DB (th∆∞·ªùng l√† string ho·∫∑c dict)
    try:
        if isinstance(raw_input, str):
            data = json.loads(raw_input)
        elif isinstance(raw_input, dict):
            data = raw_input.copy()
    except: return {}

    # L·ªõp 2: Ki·ªÉm tra c√°c key ch·ª©a JSON l·ªìng nhau th∆∞·ªùng g·∫∑p
    # Game 2 th∆∞·ªùng nh√©t d·ªØ li·ªáu v√†o key 'event_json' ho·∫∑c 'params'
    nested_keys = ['event_json', 'params', 'data', 'attributes']
    
    for key in nested_keys:
        if key in data and isinstance(data[key], str):
            try:
                inner = json.loads(data[key])
                if isinstance(inner, dict):
                    data.update(inner) # G·ªôp d·ªØ li·ªáu con ra ngo√†i
            except: pass
            
    # L·ªõp 3: X·ª≠ l√Ω Double Encode (Tr∆∞·ªùng h·ª£p chu·ªói b·ªã m√£ h√≥a 2 l·∫ßn)
    if isinstance(data, str): 
        try: data = json.loads(data)
        except: pass
        
    return data if isinstance(data, dict) else {}

def get_app_config(cur, app_id):
    """
    H√†m l·∫•y c·∫•u h√¨nh ƒë·ªông t·ª´ Database.
    N·∫øu kh√¥ng c√≥, tr·∫£ v·ªÅ c·∫•u h√¨nh m·∫∑c ƒë·ªãnh (Fallback) ƒë·ªÉ tr√°nh l·ªói.
    """
    try:
        cur.execute("SELECT config_json FROM analytics_config WHERE app_id = %s", (app_id,))
        row = cur.fetchone()
        if row and row['config_json']:
            return row['config_json']
    except Exception as e:
        print(f"Config Warning: {e}")
        # Quan tr·ªçng: N·∫øu query l·ªói, ph·∫£i rollback ƒë·ªÉ kh√¥ng k·∫πt transaction sau n√†y
        if cur.connection:
            cur.connection.rollback()

    # C·∫•u h√¨nh M·∫∑c ƒë·ªãnh (Fallback) n·∫øu ch∆∞a setup DB
    return {
        "events": {
            "start": ["missionStart", "missionStart_Daily", "missionStart_WeeklyQuestTutor"],
            "win": ["missionComplete", "missionComplete_Daily", "missionComplete_WeeklyQuestTutor"],
            "progress": ["missionProgress"],
            "fail": ["missionFail", "missionFail_Daily", "missionFail_WeeklyQuestTutor"],
            "transaction": {
                "real_currency": ["iapSuccess", "firstIAP"], # <--- ƒê√£ th√™m d·∫•u ph·∫©y
                "virtual_currency_exclude": ["iapSuccess", "firstIAP", "iapPurchase", "priceSpendLevel"], # <--- ƒê√£ th√™m d·∫•u ph·∫©y
                "offer_and_reward": ["FirstReward", "adsRewardComplete", "iapOfferGet", "dailyReward"]
            }
        },
        "boosters": [ # <--- S·ª≠a ngo·∫∑c nh·ªçn { th√†nh ngo·∫∑c vu√¥ng [
            {"key": "booster_Hammer", "name": "Hammer üî®", "type": "booster"},
            {"key": "booster_Magnet", "name": "Magnet üß≤", "type": "booster"},
            {"key": "booster_Add", "name": "Add Moves ‚ûï", "type": "booster"},
            {"key": "booster_Unlock", "name": "Unlock üîì", "type": "booster"},
            {"key": "booster_Clear", "name": "Clear üßπ", "type": "booster"},
            {"key": "revive_boosterClear", "name": "Revive üíñ", "type": "revive"}
        ], # <--- S·ª≠a ngo·∫∑c nh·ªçn } th√†nh ngo·∫∑c vu√¥ng [
        "currency": {
            "real": ["VND", "USD", "‚Ç´", "$"], # <--- ƒê√£ th√™m d·∫•u ph·∫©y
            "virtual": ["Coin"]
        }
    }

def smart_parse_json(raw_input):
    """
    H√†m th√¥ng minh ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªùng h·ª£p JSON b·ªã l·ªìng 2 l·ªõp.
    V√≠ d·ª•: "{\"event_json\": \"{\\\"levelID\\\": 1...}\"}"
    """
    if not raw_input: 
        return {}
    
    try:
        # L·ªõp 1: N·∫øu l√† string th√¨ parse ra dict, n·∫øu l√† dict r·ªìi th√¨ gi·ªØ nguy√™n
        parsed_data = json.loads(raw_input) if isinstance(raw_input, str) else raw_input
        
        # L·ªõp 2: Ki·ªÉm tra xem b√™n trong c√≥ key 'event_json' ch·ª©a string JSON n·ªØa kh√¥ng (L·ªói double encode)
        if isinstance(parsed_data, dict) and 'event_json' in parsed_data:
            inner_value = parsed_data['event_json']
            if isinstance(inner_value, str):
                try:
                    inner_json = json.loads(inner_value)
                    # G·ªôp d·ªØ li·ªáu b√™n trong ra ngo√†i (Flatten)
                    parsed_data.update(inner_json)
                except:
                    pass # N·∫øu kh√¥ng parse ƒë∆∞·ª£c l·ªõp trong th√¨ th√¥i
                    
        return parsed_data
    except Exception:
        return {}

def get_db():
    try:
        conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS)
        return conn
    except Exception as e:
        print("‚ùå L·ªñI K·∫æT N·ªêI DB:", e)
        return None

# ==========================================
# PH·∫¶N 1: CORE FUNCTIONS (T·∫†O JOB & C·∫¨P NH·∫¨T)
# ==========================================

def create_etl_job(app_id, date_since, date_until):
    conn = get_db()
    if not conn: return
    cur = conn.cursor()
    try:
        # Check tr√πng: N·∫øu ƒë√£ c√≥ job ƒëang ch·ªù/ch·∫°y c√πng khung gi·ªù th√¨ th√¥i
        cur.execute("""
            SELECT id FROM etl_jobs 
            WHERE app_id = %s AND date_since = %s AND status IN ('pending', 'processing')
        """, (app_id, date_since))
        if cur.fetchone(): 
            return # ƒê√£ c√≥ job r·ªìi, kh√¥ng t·∫°o th√™m

        cur.execute("""
            INSERT INTO etl_jobs (app_id, date_since, date_until, status, retry_count, message, created_at)
            VALUES (%s, %s, %s, 'pending', 0, 'Scheduled Auto', NOW())
        """, (app_id, date_since, date_until))
        conn.commit()
        print(f"üé´ Auto: ƒê√£ t·∫°o v√© Job cho App {app_id}")
    except Exception as e:
        print(f"‚ùå Auto Error: {e}")
    finally:
        cur.close()
        conn.close()

def update_job_status(job_id, status, message=None, inc_retry=False):
    conn = get_db()
    if not conn: return
    cur = conn.cursor()
    try:
        sql = "UPDATE etl_jobs SET status = %s, updated_at = NOW(), message = %s"
        if inc_retry: sql += ", retry_count = retry_count + 1"
        sql += " WHERE id = %s"
        cur.execute(sql, (status, message, job_id))
        conn.commit()
    finally:
        cur.close()
        conn.close()

# ==========================================
# PH·∫¶N 2: WORKER TH√îNG MINH (ƒê√É S·ª¨A L·ªñI TIME & INSERT TR∆Ø·ªöC)
# ==========================================
# --- H√ÄM PH·ª§ TR·ª¢: GHI LOG V√ÄO DB ---
def append_log_to_db(hist_id, new_log_line):
    """N·ªëi th√™m log v√†o d√≤ng l·ªãch s·ª≠ ƒëang ch·∫°y"""   
    if not hist_id: return
    try:
        conn = get_db()
        cur = conn.cursor()
        # D√πng to√°n t·ª≠ || ƒë·ªÉ n·ªëi chu·ªói trong PostgreSQL
        timestamp = datetime.now().strftime('%H:%M:%S')
        log_entry = f"\n[{timestamp}] {new_log_line}"
        
        cur.execute("""
            UPDATE job_history 
            SET logs = COALESCE(logs, '') || %s, updated_at = NOW()
            WHERE id = %s
        """, (log_entry, hist_id))
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"‚ùå Error appending log: {e}")

def transform_events_to_level_analytics(app_id, events):
    """
    [UPDATED] Transform missionStart / missionComplete / missionFail
    S·ª≠ d·ª•ng smart_parse_json ƒë·ªÉ x·ª≠ l√Ω l·ªói l·ªìng JSON.
    """
    if not events:
        return

    conn = get_db()
    cur = conn.cursor()

    # 1. Gom event theo (user_id, level_id)
    sessions = {}

    for e in events:
        try:
            event_name = e.get("event_name")
            raw_json = smart_parse_json(e.get("event_json"))
            level_id = (raw_json.get("levelID") or 
                        raw_json.get("missionID") or 
                        raw_json.get("level_display") or 
                        raw_json.get("level_display_origin"))
            user_id = (raw_json.get("userID") or "Guest" or
                       raw_json.get("uuid"))
            # N·∫øu v·∫´n kh√¥ng l·∫•y ƒë∆∞·ª£c level_id, b·ªè qua event n√†y
            if not level_id: continue
            session_key = f"{user_id}_{level_id}"
            # Timestamp x·ª≠ l√Ω an to√†n
            try:
                ts_val = int(e.get("event_timestamp"))
                ts = datetime.fromtimestamp(ts_val)
            except:
                ts = datetime.now()

            if session_key not in sessions:
                sessions[session_key] = {
                    "app_id": app_id,
                    "user_id": user_id,
                    "level_id": level_id,
                    "start_time": None,
                    "end_time": None,
                    "status": "DROP",
                    "total_cost": 0
                }

            s = sessions[session_key]

            # Logic t√≠nh to√°n gi·ªØ nguy√™n, ch·ªâ ƒë·∫£m b·∫£o raw_json ƒë√£ s·∫°ch
            if event_name == "missionStart":
                s["start_time"] = ts

            elif event_name == "missionComplete":
                s["end_time"] = ts
                s["status"] = "WIN"
                for k, v in raw_json.items():
                    if k.startswith("booster_") or k.startswith("revive_"):
                        try: s["total_cost"] += int(v)
                        except: pass

            elif event_name == "missionFail":
                s["end_time"] = ts
                s["status"] = "FAIL"
                for k, v in raw_json.items():
                    if k.startswith("booster_") or k.startswith("revive_"):
                        try: s["total_cost"] += int(v)
                        except: pass

        except Exception as ex:
            print(f"Transform error skipping row: {ex}")

    # 2. Insert v√†o level_analytics
    for s in sessions.values():
        start_time = s["start_time"]
        end_time = s["end_time"]
        duration = 0
        if start_time and end_time:
            duration = int((end_time - start_time).total_seconds())

        # Ch·ªâ insert n·∫øu c√≥ d·ªØ li·ªáu h·ª£p l·ªá (Tr√°nh r√°c)
        try:
            cur.execute("""
                INSERT INTO level_analytics
                (app_id, session_id, user_id, level_name, status, duration, start_time, total_cost, created_at)
                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,NOW())
            """, (
                s["app_id"],
                f"{s['user_id']}_{s['level_id']}",
                s["user_id"],
                f"Level {s['level_id']}", # Format t√™n Level ƒë·∫πp h∆°n
                s["status"],
                duration,
                start_time,
                s["total_cost"]
            ))
        except Exception as insert_err:
            print(f"Insert Analytics Error: {insert_err}")

    conn.commit()
    conn.close()

def execute_job_logic(job_id, app_id, retry_count, run_type, retry_job_id):
    """
    Worker V107: Smart Retry (Ch·ªâ Retry 202, L·ªói 4xx d·ª´ng ngay)
    """
    hist_id = None
    try:
        conn = get_db()
        if not conn: return
        cur = conn.cursor()
        
        cur.execute("INSERT INTO job_history (app_id, start_time, status, run_type, logs, total_events) VALUES (%s, NOW(), 'Running', 'schedule', '', 0) RETURNING id", (app_id,))
        hist_id = cur.fetchone()[0]
        conn.commit()
        
        def log(msg):
            ts = datetime.now().strftime("[%H:%M:%S]")
            print(f"{ts} [App {app_id}] {msg}")
            append_log_to_db(hist_id, msg)

        log(f"‚ñ∂Ô∏è Start Job #{job_id} (Attempt {retry_count})...")

        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT * FROM etl_jobs WHERE id = %s", (job_id,))
        job = cur.fetchone()
        conn.close() 

        conn_thread = get_db()
        cur_thread = conn_thread.cursor(cursor_factory=RealDictCursor)
        cur_thread.execute("SELECT * FROM apps WHERE id = %s", (app_id,))
        app_info = cur_thread.fetchone()
        
        if not app_info:
            log("‚ùå App missing."); update_job_status(job_id, 'failed'); return

        clean_app_id = str(app_info['app_id']).strip()
        clean_token = str(app_info['api_token']).strip()
        
        try:
            u_start = datetime.strptime(str(job['date_since']), '%Y-%m-%d %H:%M:%S')
            u_end = datetime.strptime(str(job['date_until']), '%Y-%m-%d %H:%M:%S')
            vn_start = u_start + timedelta(hours=7)
            vn_end = u_end + timedelta(hours=7)
            log(f"üïí Scanning Window: VN[{vn_start.strftime('%H:%M')} - {vn_end.strftime('%H:%M')}]")
        except: pass

        url = "https://api.appmetrica.yandex.com/logs/v1/export/events.json"
        params = {
            "application_id": clean_app_id,
            "date_since": str(job['date_since']), 
            "date_until": str(job['date_until']),
            "fields": "event_name,event_timestamp,event_json",
            "limit": 1000000 
        }
        headers = {"Authorization": f"OAuth {clean_token}"}
        
        # --- LOGIC RETRY TH√îNG MINH ---
        max_attempts = 18 
        poll_interval = 180 # 180 gi√¢y cho 202
        
        for attempt in range(1, max_attempts + 1):
            if JOB_STOP_EVENTS.get(hist_id) and JOB_STOP_EVENTS[hist_id].is_set():
                log("üõë Stopped."); update_job_status(job_id, 'cancelled'); break
            
            log(f"üì° Requesting AppMetrica ({attempt}/{max_attempts})...")
            
            try:
                response = requests.get(url, params=params, headers=headers, stream=True, timeout=600)
                
                if response.status_code == 200:
                    # SUCCESS
                    data = response.json()
                    events = data.get('data', [])
                    count = len(events)
                    log(f"‚úÖ Downloaded {count} events. Saving...")
                    
                    conn_ins = get_db(); cur_ins = conn_ins.cursor()
                    vals = []
                    for e in events:
                        evt_json = json.dumps(e)
                        try: ts = datetime.fromtimestamp(int(e.get('event_timestamp')))
                        except: ts = datetime.now()
                        vals.append((app_id, e.get('event_name'), evt_json, 1, ts))
                    
                    if vals:
                        cur_ins.executemany("INSERT INTO event_logs (app_id, event_name, event_json, count, created_at) VALUES (%s, %s, %s, %s, %s)", vals)
                    conn_ins.commit(); conn_ins.close()
                    
                    try:
                        transform_events_to_level_analytics(app_id, events)
                    except Exception as te: log(f"‚ö†Ô∏è Transform: {te}")

                    log(f"üéâ Success. Imported {count}.")
                    update_job_status(job_id, 'completed', f"OK. {count} events.")
                    
                    conn_fin = get_db(); cur_fin = conn_fin.cursor()
                    cur_fin.execute("UPDATE job_history SET end_time=NOW(), status='Success', total_events=%s WHERE id=%s", (count, hist_id))
                    conn_fin.commit(); conn_fin.close()
                    break 
                
                elif response.status_code == 202:
                    # HTTP 202: ƒê·ª£i 180s r·ªìi th·ª≠ l·∫°i (ƒê√ÇY L√Ä TR∆Ø·ªúNG H·ª¢P DUY NH·∫§T ƒê∆Ø·ª¢C RETRY)
                    if attempt < max_attempts:
                        log(f"‚è≥ Server 202 (Preparing). Waiting {poll_interval}s...")
                        time.sleep(poll_interval)
                        continue 
                    else:
                        log("‚ùå Timeout 202.")
                        update_job_status(job_id, 'failed', "Timeout 202")
                        conn_fin = get_db(); cur_fin = conn_fin.cursor()
                        cur_fin.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
                        conn_fin.commit(); conn_fin.close()
                
                elif response.status_code in [400, 401, 403]:
                    # L·ªñI CLIENT (Sai Token, Sai ID...) -> D·ª™NG NGAY
                    log(f"‚ùå FATAL ERROR {response.status_code}: {response.text[:100]}")
                    update_job_status(job_id, 'failed', f"Fatal {response.status_code}")
                    conn_fin = get_db(); cur_fin = conn_fin.cursor()
                    cur_fin.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
                    conn_fin.commit(); conn_fin.close()
                    break # Break ngay l·∫≠p t·ª©c, kh√¥ng Retry
                
                else:
                    # L·ªói kh√°c (500, 502...) -> Th·ª≠ l·∫°i nh·∫π nh√†ng
                    log(f"‚ùå Server Error {response.status_code}. Retry in 10s...")
                    time.sleep(10)

            except Exception as req_err:
                log(f"‚ùå Network Error: {req_err}")
                time.sleep(10)
        
        else:
            # H·∫øt v√≤ng l·∫∑p
            if JOB_STOP_EVENTS.get(hist_id) and not JOB_STOP_EVENTS[hist_id].is_set():
                log("‚ùå Job Failed (Max Retries).")
                update_job_status(job_id, 'failed', "Max Retries")
                conn_fin = get_db(); cur_fin = conn_fin.cursor()
                cur_fin.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
                conn_fin.commit(); conn_fin.close()

    except Exception as e:
        print(f"Critical: {e}")
        if hist_id: append_log_to_db(hist_id, f"‚ùå Crash: {e}")
    finally:
        print(f"üîì App {app_id} Free.")
        unlock_app(app_id)

def worker_process_jobs():
    """
    DISPATCHER V104: Log picking up job
    """
    try:
        conn = get_db()
        if not conn: return
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        cur.execute("""
            SELECT * FROM etl_jobs 
            WHERE status = 'pending' 
            ORDER BY created_at ASC
        """)
        jobs = cur.fetchall()
        cur.close(); conn.close()

        for job in jobs:
            app_id = job['app_id']
            job_id = job['id']
            
            if try_lock_app(app_id):
                # In log Dispatcher
                ts = datetime.now().strftime("[%H:%M:%S]")
                print(f"{ts} ‚ñ∂Ô∏è Worker picking up Job #{job_id} (Parallel Start)")
                
                update_job_status(job_id, 'processing', 'Worker starting...')

                t = threading.Thread(target=execute_job_logic, args=(
                    job_id, 
                    app_id, 
                    job['retry_count'], 
                    job.get('run_type'), 
                    job.get('retry_job_id')
                ))
                t.start()
            else:
                pass

    except Exception as e:
        print(f"‚ùå Dispatcher Error: {e}")

def run_worker_loop():
    print("üöÄ Worker Loop Started...")
    while True:
        try:
            worker_process_jobs()
        except Exception as e:
            print(f"‚ùå Worker Loop Error: {e}")
        time.sleep(60)

def run_scheduler_loop():
    print("üöÄ Auto Scheduler V103.5 (Strict Config Mode) Started...")
    while True:
        try:
            now = datetime.now()
            conn = get_db()
            if not conn:
                time.sleep(60); continue
                
            cur = conn.cursor(cursor_factory=RealDictCursor)
            cur.execute("SELECT * FROM apps WHERE is_active = true") 
            apps = cur.fetchall()

            for app in apps:
                app_id = app['id']
                
                # --- [ƒê·ªåC C·∫§U H√åNH T·ª™ TAB SETTINGS] ---
                # 1. L·∫•y chu k·ª≥ (V√≠ d·ª•: 60 ph√∫t)
                interval = app.get('interval_minutes', 60) or 60
                
                # 2. L·∫•y M·ªëc Th·ªùi Gian B·∫Øt ƒê·∫ßu (V√≠ d·ª•: "14:15")
                sch_str = app.get('schedule_time', '00:00')
                
                # --- THU·∫¨T TO√ÅN T√çNH GI·ªú CH·∫†Y ---
                # Parse gi·ªù v√† ph√∫t t·ª´ c·∫•u h√¨nh (h=14, m=15)
                try: h, m = map(int, sch_str.split(':'))
                except: h=0; m=0
                
                # T·∫°o m·ªëc Anchor h√¥m nay d·ª±a tr√™n gi·ªù c√†i ƒë·∫∑t (H√¥m nay l√∫c 14:15)
                anchor = now.replace(hour=h, minute=m, second=0, microsecond=0)
                
                # N·∫øu m·ªëc n√†y n·∫±m trong t∆∞∆°ng lai -> L√πi v·ªÅ h√¥m qua ƒë·ªÉ l√†m g·ªëc t√≠nh to√°n
                if anchor > now: 
                    anchor -= timedelta(days=1)
                
                # T√≠nh s·ªë chu k·ª≥ ƒë√£ tr√¥i qua k·ªÉ t·ª´ m·ªëc g·ªëc
                delta_seconds = (now - anchor).total_seconds()
                cycles_passed = int(delta_seconds // (interval * 60))
                
                # T√çNH GI·ªú CH·∫†Y CH√çNH X√ÅC C·ª¶A CHU K·ª≤ HI·ªÜN T·∫†I
                # C√¥ng th·ª©c: M·ªëc C√†i ƒê·∫∑t + (S·ªë chu k·ª≥ * S·ªë ph√∫t m·ªói chu k·ª≥)
                expected_run_time = anchor + timedelta(minutes=cycles_passed * interval)
                
                # --- KI·ªÇM TRA & T·∫†O JOB ---
                # Logic: N·∫øu b√¢y gi·ªù (now) v·ª´a c√°n qua m·ªëc expected_run_time (trong v√≤ng 5 ph√∫t ƒë·ªï l·∫°i)
                # V√≠ d·ª•: Gi·ªù ch·∫°y l√† 16:15. B√¢y gi·ªù l√† 16:16 -> OK, T·∫°o job!
                
                if expected_run_time <= now <= (expected_run_time + timedelta(minutes=5)):
                    
                    # Ki·ªÉm tra xem ƒë√£ t·∫°o job cho l·∫ßn ch·∫°y n√†y ch∆∞a (tr√°nh t·∫°o tr√πng)
                    # Qu√©t t√¨m job ƒë∆∞·ª£c t·∫°o trong 10 ph√∫t g·∫ßn ƒë√¢y
                    check_start = now - timedelta(minutes=10)
                    cur.execute("""
                        SELECT count(*) as count FROM etl_jobs 
                        WHERE app_id = %s AND created_at >= %s
                    """, (app_id, check_start))
                    
                    if cur.fetchone()['count'] == 0:
                        print(f"‚è∞ [App {app_id}] ƒê√∫ng gi·ªù {expected_run_time.strftime('%H:%M')} (Theo c·∫•u h√¨nh {sch_str}). T·∫°o Job!")
                        
                        # C·∫•u h√¨nh th·ªùi gian l·∫•y d·ªØ li·ªáu (L√πi 90p, l·∫•y 60p)
                        delay_minutes = 90
                        duration_minutes = 60
                        
                        end_dt_vn = now - timedelta(minutes=delay_minutes)
                        start_dt_vn = end_dt_vn - timedelta(minutes=duration_minutes)
                        
                        # ƒê·ªïi sang UTC cho server AppMetrica
                        end_dt_utc = end_dt_vn - timedelta(hours=7)
                        start_dt_utc = start_dt_vn - timedelta(hours=7)
                        
                        # T·∫°o Job
                        create_etl_job(
                            app_id, 
                            start_dt_utc.strftime('%Y-%m-%d %H:%M:%S'), 
                            end_dt_utc.strftime('%Y-%m-%d %H:%M:%S')
                        )
            
            cur.close(); conn.close()
        except Exception as e:
            print(f"‚ùå Scheduler Error: {e}")
        
        # Ng·ªß 60s
        time.sleep(60)

# ==========================================
# PH·∫¶N 4: LOGIC CH·∫†Y TAY (MANUAL) - [UPDATED V98 PARALLEL]
# ==========================================
def perform_manual_etl(app_id, run_type='manual', is_demo=False, retry_job_id=None):
    # [THAY ƒê·ªîI QUAN TR·ªåNG] Ki·ªÉm tra kh√≥a ri√™ng c·ªßa App thay v√¨ kh√≥a h·ªá th·ªëng
    if not try_lock_app(app_id):
        print(f"‚ùå App {app_id} is BUSY (Parallel Check). Skip manual run.")
        return

    hist_id = None

    try:
        conn = get_db()
        if not conn: return
        
        # 1. T·∫†O HISTORY
        msg_start = f"üöÄ Starting {run_type.upper()} run..."
        if retry_job_id: msg_start += f" (Retry of Job #{retry_job_id})"
        
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO job_history (app_id, start_time, status, run_type, logs, total_events) 
            VALUES (%s, NOW(), 'Running', %s, %s, 0) 
            RETURNING id
        """, (app_id, run_type, f"[{datetime.now().strftime('%H:%M:%S')}] {msg_start}"))
        hist_id = cur.fetchone()[0]
        conn.commit()
        
        stop_event = threading.Event()
        JOB_STOP_EVENTS[hist_id] = stop_event

        def log(msg):
            print(f"[App {app_id}] {msg}")
            append_log_to_db(hist_id, msg)

        # 2. L·∫§Y CONFIG
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT * FROM apps WHERE id=%s", (app_id,))
        app = cur.fetchone()
        
        if not app:
            log("‚ùå Error: App ID not found."); return

        # 3. X·ª¨ L√ù NG√ÄY GI·ªú (Strict Retry V83 logic)
        date_since = None; date_until = None
        
        if run_type == 'retry' and retry_job_id:
            cur.execute("SELECT logs, start_time FROM job_history WHERE id = %s", (retry_job_id,))
            old_row = cur.fetchone()
            
            if old_row and old_row['logs']:
                logs = old_row['logs']
                # Regex V83: T√¨m b·∫•t k·ª≥ 2 chu·ªói ng√†y gi·ªù n√†o n·∫±m tr√™n c√πng 1 d√≤ng
                timestamps = re.findall(r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})", logs)
                if len(timestamps) >= 2:
                    date_since = timestamps[-2]
                    date_until = timestamps[-1]
                    log(f"üîô RETRY V83: Found timestamps: {date_since} -> {date_until}")
                elif old_row['start_time']:
                     short_times = re.findall(r"(\d{2}:\d{2})", logs)
                     if len(short_times) >= 2:
                         base_date = old_row['start_time'].strftime('%Y-%m-%d')
                         date_since = f"{base_date} {short_times[-2]}:00"
                         date_until = f"{base_date} {short_times[-1]}:00"
                         log(f"üîô RETRY V83 (Short): Reconstructed: {date_since} -> {date_until}")

            if not date_since:
                log(f"‚ùå RETRY FAILED: Cannot find timestamps in Job #{retry_job_id} logs."); return

        # 4. CH·∫†Y MANUAL (T√≠nh gi·ªù n·∫øu kh√¥ng ph·∫£i Retry)
        elif not date_since:
            now = datetime.now()
            delay = 45 if run_type == 'demo' else 90
            duration = 15 if run_type == 'demo' else 60
            
            end_dt = now - timedelta(minutes=delay)
            start_dt = end_dt - timedelta(minutes=duration)
            date_since = start_dt.strftime('%Y-%m-%d %H:%M:%S')
            date_until = end_dt.strftime('%Y-%m-%d %H:%M:%S')
            log(f"‚öôÔ∏è MANUAL MODE: {date_since} -> {date_until}")

        log(f" üïí Scanning Window: {date_since} -> {date_until}")
        
        # 5. G·ªåI API APPMETRICA
        clean_app_id = str(app['app_id']).strip()
        clean_token = str(app['api_token']).strip()
        url = "https://api.appmetrica.yandex.com/logs/v1/export/events.json"
        params = { "application_id": clean_app_id, "date_since": date_since, "date_until": date_until, "fields": "event_name,event_timestamp,event_json", "limit": 1000000 }
        headers = {"Authorization": f"OAuth {clean_token}"}
        
        status = "Failed"; total = 0
        for i in range(18): # 18 retries
            if stop_event.is_set(): status="Cancelled"; break
            log(f"üì° Requesting AppMetrica ({i+1}/18)...")
            r = requests.get(url, params=params, headers=headers)
            
            if r.status_code == 200:
                data = r.json().get('data', [])
                total = len(data)
                log(f"‚úÖ Importing {total} events...")
                
                # Insert DB
                conn2 = get_db(); cur2 = conn2.cursor()
                vals = []
                for d in data:
                    try: ts = datetime.fromtimestamp(int(d.get('event_timestamp')))
                    except: ts = datetime.now()
                    vals.append((app_id, d.get('event_name'), json.dumps(d), 1, ts))
                
                cur2.executemany("INSERT INTO event_logs (app_id, event_name, event_json, count, created_at) VALUES (%s,%s,%s,%s,%s)", vals)
                conn2.commit(); conn2.close()
                
                # Transform (Quan tr·ªçng: G·ªçi h√†m transform ƒë·ªÉ t√≠nh to√°n Level Analytics)
                try:
                    transform_events_to_level_analytics(app_id, data)
                    log(f"üîÑ Transformed analytics for {total} events.")
                except Exception as te:
                    log(f"‚ö†Ô∏è Transform Error: {te}")

                status = "Success"; log("üéâ Done."); break
            
            elif r.status_code == 202:
                log("‚è≥ 202 Waiting 180s..."); 
                if stop_event.wait(180): status="Cancelled"; break
            else:
                log(f"‚ùå Error {r.status_code}"); status="Failed"; break
        
        # Finalize
        conn3 = get_db(); cur3 = conn3.cursor()
        cur3.execute("UPDATE job_history SET end_time=NOW(), status=%s, total_events=%s WHERE id=%s", (status, total, hist_id))
        conn3.commit(); conn3.close()

    except Exception as e: log(f"‚ùå Error: {e}")
    finally:
        # [QUAN TR·ªåNG] Gi·∫£i ph√≥ng App ƒë·ªÉ n√≥ c√≥ th·ªÉ ch·∫°y job kh√°c
        unlock_app(app_id)
        if hist_id and hist_id in JOB_STOP_EVENTS: del JOB_STOP_EVENTS[hist_id]

# PH·∫¶N 5: API ENDPOINTS (ƒê√É C·∫¨P NH·∫¨T DASHBOARD)
@app.route("/monitor/history", methods=['GET'])
def get_history():
    app_id = request.args.get('app_id') 
    # [M·ªöI] L·∫•y tham s·ªë ph√¢n trang
    try:
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 30)) # M·∫∑c ƒë·ªãnh 30 d√≤ng/trang
    except:
        page = 1; limit = 30
        
    offset = (page - 1) * limit
    conn = get_db()
    if not conn: return jsonify([])
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # 1. X√¢y d·ª±ng m·ªánh ƒë·ªÅ WHERE
        where_clause = ""
        params_count = []
        if app_id: 
            where_clause = "WHERE h.app_id = %s"
            params_count.append(app_id)

        # 2. ƒê·∫øm t·ªïng s·ªë records (ƒë·ªÉ t√≠nh s·ªë trang)
        cur.execute(f"SELECT COUNT(*) as total FROM job_history h {where_clause}", tuple(params_count))
        total_records = cur.fetchone()['total']
        total_pages = (total_records + limit - 1) // limit

        # 3. L·∫•y d·ªØ li·ªáu ph√¢n trang
        query = f"""
            SELECT h.*, a.name as app_name 
            FROM job_history h 
            JOIN apps a ON h.app_id = a.id 
            {where_clause}
            ORDER BY h.start_time DESC 
            LIMIT %s OFFSET %s
        """
        # Copy params t·ª´ count sang v√† th√™m limit/offset
        params_data = params_count + [limit, offset]
        
        cur.execute(query, tuple(params_data))
        res = cur.fetchall()

        # --- [FIX L·ªñI TIMEZONE V√Ä DURATION] ---
        # T√≠nh to√°n Duration v√† Format l·∫°i Time ƒë·ªÉ tr√°nh Frontend t·ª± c·ªông +7 ti·∫øng
        for row in res:
            # 1. T√≠nh Duration
            duration_str = "..."
            if row['start_time'] and row['end_time']:
                diff = row['end_time'] - row['start_time']
                total_seconds = int(diff.total_seconds())
                minutes = total_seconds // 60
                seconds = total_seconds % 60
                duration_str = f"{minutes} min {seconds} sec"
            elif row['status'] == 'Running':
                 duration_str = "Running..."
            
            row['duration'] = duration_str

            # 2. Fix Timezone: Chuy·ªÉn datetime th√†nh string c·ª©ng
            # ƒê·ªÉ Frontend hi·ªÉn th·ªã y nguy√™n gi·ªù c·ªßa Server (Local VN)
            if row['start_time']:
                row['start_time'] = row['start_time'].strftime('%d/%m/%Y %H:%M:%S')
            if row['end_time']:
                row['end_time'] = row['end_time'].strftime('%d/%m/%Y %H:%M:%S')

        return jsonify({
            "data": res,
            "pagination": {
                "current_page": page,
                "total_pages": total_pages,
                "total_records": total_records
            }
        })
    except Exception as e:
        print(f"History Error: {e}")
        return jsonify({"data": [], "pagination": {}})    
    finally: conn.close()

@app.route("/monitor/purge", methods=['DELETE'])
def purge_history():
    conn = get_db()
    try:
        cur = conn.cursor()
        cur.execute("DELETE FROM job_history")
        conn.commit()
        return jsonify({"msg": "History Cleared"})
    finally: conn.close()

@app.route("/apps", methods=['GET', 'POST'])
def handle_apps():
    conn = get_db()
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        if request.method == 'GET':
            cur.execute("SELECT * FROM apps ORDER BY id ASC")
            return jsonify(cur.fetchall())
        else:
            d = request.json
            cur.execute("INSERT INTO apps (name, app_id, api_token, is_active, schedule_time, interval_minutes) VALUES (%s, %s, %s, %s, %s, %s)", 
                        (d['name'], d['app_id'], d['api_token'], d['is_active'], d.get('schedule_time', '12:00'), d.get('interval_minutes', 60)))
            conn.commit()
            return jsonify({"msg": "Created"})
    finally: conn.close()

@app.route("/apps/<int:id>", methods=['PUT', 'DELETE'])
def update_app(id):
    conn = get_db()
    try:
        cur = conn.cursor()
        if request.method == 'PUT':
            d = request.json
            cur.execute("UPDATE apps SET name=%s, app_id=%s, api_token=%s, is_active=%s, schedule_time=%s, interval_minutes=%s WHERE id=%s", 
                        (d['name'], d['app_id'], d['api_token'], d['is_active'], d.get('schedule_time', '12:00'), d.get('interval_minutes', 60), id))
            conn.commit()
            return jsonify({"msg": "Updated"})
        elif request.method == 'DELETE':
            cur.execute("DELETE FROM event_logs WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM job_history WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM etl_jobs WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM apps WHERE id=%s", (id,))
            conn.commit()
            return jsonify({"msg": "Deleted"})
    finally: conn.close()

@app.route("/etl/run/<int:app_id>", methods=['POST'])
def run_etl_api(app_id):
    # --- CODE M·ªöI ---
    data = request.json
    run_type = data.get('run_type', 'manual') # L·∫•y lo·∫°i ch·∫°y (manual/retry/demo)
    retry_job_id = data.get('retry_job_id')   # L·∫•y ID c·ªßa job c≈© n·∫øu l√† retry
    
    is_demo = (run_type == 'demo')
    
    if is_system_busy():
         return jsonify({"status": "error", "message": "System is busy processing another job. Please skip this cycle."}), 409

    # Truy·ªÅn th√™m retry_job_id v√†o h√†m x·ª≠ l√Ω
    threading.Thread(target=perform_manual_etl, args=(app_id, run_type, is_demo, retry_job_id)).start()
    return jsonify({"status": "started", "mode": run_type})

@app.route("/dashboard/<int:app_id>", methods=['GET'])
def get_dashboard(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB Connection failed"}), 500
    
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # 1. KH·ªûI T·∫†O BI·∫æN AN TO√ÄN (FIX L·ªñI 'variable not defined')
        booster_config_list = [] 
        real_events = ["iapSuccess", "firstIAP", "iapPurchase"]
        start_events = ["missionStart", "missionStart_Daily", "level_start", "level_loading_start", "level_first_start"]
        fail_events = ["missionFail", "missionFail_Daily", "level_fail", "level_lose"]
        
        try:
            config = get_app_config(cur, app_id)
            if config:
                # C·∫≠p nh·∫≠t n·∫øu config c√≥ d·ªØ li·ªáu
                c_booster = config.get('boosters')
                if isinstance(c_booster, list): booster_config_list = c_booster
                
                c_real = config.get('events', {}).get('transaction', {}).get('real_currency')
                if c_real: real_events = c_real
        except: pass

        # L·∫•y danh s√°ch key booster
        booster_keys = [b['key'] for b in booster_config_list if isinstance(b, dict) and 'key' in b]
        if not booster_keys:
            booster_keys = ["booster_Hammer", "booster_Magnet", "booster_Add", "booster_Unlock", "booster_Clear", "revive_boosterClear", "booster_bubble", "booster_shuffle", "booster_ufo"]

        # 2. FILTER TIME
        where_clause = "WHERE app_id = %s"
        params = [app_id]
        if start_date: 
            where_clause += " AND created_at >= %s"
            params.append(start_date + " 00:00:00")
        if end_date: 
            where_clause += " AND created_at <= %s"
            params.append(end_date + " 23:59:59")

        # 3. DOANH THU (Regex Extract)
        cur.execute(r"""
            SELECT COALESCE(SUM(
                COALESCE(SUBSTRING(event_json FROM '"coin_spent"\s*:\s*"?(\d+)"?')::numeric, 0)
            ), 0)::int as real_revenue
            FROM event_logs
            """ + where_clause + r""" 
            AND event_name = ANY(%s) 
        """, tuple(params + [real_events]))
        real_revenue = cur.fetchone()['real_revenue']

        # 4. T·ªîNG TI√äU COIN (Regex Extract)
        cur.execute(r"""
            SELECT SUM(
                 COALESCE(SUBSTRING(event_json FROM '"coin_spent"\s*:\s*"?(\d+)"?')::int, 0)
            )::int as virtual_sink
            FROM event_logs
            """ + where_clause + r"""
            AND NOT (event_name = ANY(%s))
        """, tuple(params + [real_events]))
        virtual_sink = cur.fetchone()['virtual_sink'] or 0

        # 5. TOTAL PLAYS
        cur.execute(f"SELECT COUNT(*)::int as count FROM event_logs {where_clause} AND event_name = ANY(%s)", tuple(params + [start_events]))
        total_plays = cur.fetchone()['count'] or 0

        # 6. FAIL RATE
        cur.execute(f"SELECT COUNT(*)::int as count FROM event_logs {where_clause} AND event_name = ANY(%s)", tuple(params + [fail_events]))
        real_fail_count = cur.fetchone()['count'] or 0
        fail_rate = round((real_fail_count / total_plays) * 100, 1) if total_plays > 0 else 0.0

        # 7. CHART MAIN
        cur.execute(f"SELECT event_name as name, COUNT(*)::int as value FROM event_logs {where_clause} GROUP BY event_name ORDER BY value DESC", tuple(params))
        chart_data = cur.fetchall()

        # 8. BOOSTER REVENUE (Manual Count b·∫±ng SQL)
        # V√¨ Regex trong Group By ph·ª©c t·∫°p, ta l·∫•y raw v·ªÅ x·ª≠ l√Ω m·ªôt ch√∫t ho·∫∑c d√πng count ƒë∆°n gi·∫£n
        booster_stats = []
        if booster_keys:
            # Map gi√°
            PRICE_MAP = {}
            NAME_MAP = {}
            for b in booster_config_list:
                if isinstance(b, dict):
                    k = b.get('key')
                    PRICE_MAP[k] = b.get('price', 100)
                    NAME_MAP[k] = b.get('name', k)

            # Query ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa key trong chu·ªói JSON
            # L∆∞u √Ω: C√°ch n√†y ƒë·∫øm s·ªë d√≤ng c√≥ ch·ª©a key ƒë√≥
            for key in booster_keys:
                # Regex t√¨m "key": so_luong (s·ªë l∆∞·ª£ng > 0)
                # V√≠ d·ª•: "booster_Hammer": 1
                cur.execute(r"""
                    SELECT COUNT(*) as cnt 
                    FROM event_logs 
                    """ + where_clause + r""" 
                    AND event_json ~ (%s || '\s*:\s*[1-9]')
                """, tuple(params + [key]))
                
                count = cur.fetchone()['cnt']
                if count > 0:
                    price = PRICE_MAP.get(key, 100)
                    name = NAME_MAP.get(key, key)
                    booster_stats.append({
                        "name": name,
                        "value": count,
                        "revenue": count * price,
                        "price": price
                    })
            
            booster_stats.sort(key=lambda x: x['revenue'], reverse=True)

        return jsonify({
            "success": True,
            "overview": {
                "cards": {
                    "revenue": real_revenue,      
                    "active_users": total_plays,
                    "avg_fail_rate": fail_rate,
                    "total_spent": virtual_sink  
                },
                "chart_main": chart_data,
                "booster_chart": booster_stats
            }
        })

    except Exception as e:
        print(f"Error dashboard V93: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

@app.route("/api/levels/<int:app_id>", methods=['GET'])
def get_levels(app_id):
    conn = get_db()
    if not conn: return jsonify([])
    try:
        cur = conn.cursor()
        # L·∫•y TO√ÄN B·ªò d·ªØ li·ªáu ƒë·ªÉ l·ªçc ch√≠nh x√°c (Kh√¥ng d√πng LIMIT)
        cur.execute("SELECT event_json FROM event_logs WHERE app_id = %s", (app_id,))
        rows = cur.fetchall()
        
        levels = set()
        for r in rows:
            # D√πng m≈©i khoan v·∫°n nƒÉng V95
            data = universal_flatten(r[0])
            
            # T√¨m Level ID ·ªü m·ªçi key c√≥ th·ªÉ (Game 1 & 2)
            lvl = data.get('levelID') or data.get('level_display') or data.get('missionID')
            
            if lvl is not None:
                # L·ªçc l·∫•y s·ªë
                digits = ''.join(filter(str.isdigit, str(lvl)))
                if digits:
                    l = int(digits)
                    if l <= 2000: levels.add(l)
        
        sorted_levels = sorted(list(levels))
        return jsonify([str(l) for l in sorted_levels])
    except Exception as e:
        print(f"Error get_levels V95: {e}")
        return jsonify([])
    finally: conn.close()

@app.route("/dashboard/<int:app_id>/level-detail", methods=['GET'])
def get_level_detail(app_id):
    level_id = request.args.get('level_id')
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')
    
    # Default return ƒë·ªÉ ch·ªëng crash
    default_resp = {
        "success": True, 
        "metrics": {"total_plays":0, "win_rate":0, "arpu":0, "avg_balance":0, "top_item":"None"},
        "funnel": [], "booster_usage": [], "cost_distribution": [],
        "logs": {"data": [], "pagination": {"current": 1, "total_pages": 0, "total_records": 0}}
    }

    try: page = int(request.args.get('page', 1)); limit = int(request.args.get('limit', 50))
    except: page=1; limit=50
    offset = (page - 1) * limit

    conn = get_db()
    if not conn: return jsonify(default_resp), 500
    
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # 1. SETUP CONFIG & MAP GI√Å
        # Hardcode danh s√°ch Item c·ªßa Game 1 ƒë·ªÉ ƒë·∫£m b·∫£o hi·ªán t√™n ƒë·∫πp
        PRICE_MAP = {
            "Hammer": 120, "Magnet": 80, "Add": 60, "Unlock": 190, "Clear": 120, "Revive": 190,
            "booster_Hammer": 120, "booster_Magnet": 80, "booster_Add": 60, "booster_Unlock": 190
        }
        DISPLAY_MAP = {
            "Hammer": "Hammer üî®", "Magnet": "Magnet üß≤", "Add": "Add Moves ‚ûï", 
            "Unlock": "Unlock üîì", "Clear": "Clear üßπ", "Revive": "Revive üíñ"
        }
        
        # C·ªë g·∫Øng l·∫•y th√™m t·ª´ DB n·∫øu c√≥
        try:
            cfg = get_app_config(cur, app_id)
            if cfg and 'boosters' in cfg:
                for b in cfg['boosters']:
                    if isinstance(b, dict):
                        k = b.get('key',''); nm = b.get('name',''); pr = b.get('price', 100)
                        cl = k.replace('booster_', '').replace('revive_', '')
                        PRICE_MAP[cl] = pr; PRICE_MAP[k] = pr
                        DISPLAY_MAP[cl] = nm; DISPLAY_MAP[k] = nm
        except: pass

        # 2. B·ªò L·ªåC SQL "L·ªéNG" (LOOSE FILTER)
        # Ch·ªâ c·∫ßn th·∫•y key level v√† s·ªë level n·∫±m tr√™n c√πng 1 d√≤ng l√† l·∫•y v·ªÅ.
        # Python s·∫Ω l·ªçc ch√≠nh x√°c sau. C√°ch n√†y nhanh v√† kh√¥ng bao gi·ªù s√≥t.
        where = "WHERE app_id = %s"
        params = [app_id]
        
        # Regex: T√¨m (levelID ho·∫∑c missionID...) theo sau l√† b·∫•t k·ª≥ k√Ω t·ª± n√†o, r·ªìi ƒë·∫øn s·ªë Level
        # V√≠ d·ª• b·∫Øt ƒë∆∞·ª£c: "levelID": 90, "levelID": "90", \"levelID\":90
        level_regex = f'(levelID|level_display|missionID).*?{level_id}'
        where += f" AND event_json ~ '{level_regex}'"

        if start_date: where += " AND created_at >= %s"; params.append(start_date + " 00:00:00")
        if end_date: where += " AND created_at <= %s"; params.append(end_date + " 23:59:59")

        # 3. L·∫§Y D·ªÆ LI·ªÜU
        cur.execute(f"SELECT created_at, event_name, event_json FROM event_logs {where} ORDER BY created_at DESC", tuple(params))
        rows = cur.fetchall()

        # 4. X·ª¨ L√ù PYTHON (CH√çNH X√ÅC)
        target_lvl = str(level_id)
        filtered_rows = []
        
        metrics = {"start":0, "win":0, "fail":0, "spend":0, "rev":0}
        cost_dist = {"win_cost": 0, "fail_cost": 0}
        b_counts = {}
        
        # Danh s√°ch Event c·ªßa Game 1 & 2
        start_set = {"missionStart", "missionStart_Daily", "level_start", "level_loading_start", "missionStart_WeeklyQuestTutor"}
        win_set = {"missionComplete", "missionComplete_Daily", "level_win", "missionComplete_WeeklyQuestTutor"}
        fail_set = {"missionFail", "missionFail_Daily", "level_fail", "level_lose", "missionFail_WeeklyQuestTutor"}

        for r in rows:
            # D√πng h√†m universal_flatten (ƒê√£ c√≥ t·ª´ V95)
            data = universal_flatten(r['event_json'])
            
            # Ki·ªÉm tra Level CH√çNH X√ÅC t·∫°i ƒë√¢y
            # L·∫•y t·∫•t c·∫£ c√°c tr∆∞·ªùng c√≥ th·ªÉ l√† level
            l_val = str(data.get('levelID') or data.get('level_display') or data.get('missionID') or "")
            
            # So s√°nh string: "90" == "90"
            if l_val != target_lvl: continue
            
            # L∆∞u l·∫°i
            r_dict = dict(r); r_dict['parsed'] = data
            filtered_rows.append(r_dict)
            
            evt = r['event_name']
            
            # T√≠nh Metrics
            if evt in start_set: metrics['start'] += 1
            elif evt in win_set: metrics['win'] += 1
            elif evt in fail_set: metrics['fail'] += 1
            
            # T√≠nh Ti·ªÅn
            money = int(data.get('coin_spent') or data.get('coin_cost') or 0)
            if money > 0:
                metrics['spend'] += 1
                metrics['rev'] += money
                if evt in win_set: cost_dist['win_cost'] += money
                elif evt in fail_set: cost_dist['fail_cost'] += money

            # T√≠nh Booster
            for k, v in data.items():
                # Game 1 th∆∞·ªùng d√πng key: booster_Hammer, revive_boosterClear
                if ('booster' in k or 'revive' in k) and str(v).isdigit() and int(v) > 0:
                    clean = k.replace('booster_', '').replace('revive_', '')
                    b_counts[clean] = b_counts.get(clean, 0) + int(v)

        # 5. T·ªîNG H·ª¢P K·∫æT QU·∫¢
        real_plays = metrics['win'] + metrics['fail']
        if real_plays == 0: real_plays = metrics['start'] # Fallback n·∫øu ch∆∞a c√≥ k·∫øt qu·∫£
        
        win_rate = round((metrics['win'] / real_plays)*100, 1) if real_plays > 0 else 0
        
        b_list = []
        for k, c in b_counts.items():
            nm = DISPLAY_MAP.get(k, k.capitalize())
            pr = PRICE_MAP.get(k, 100)
            b_list.append({"item_name": nm, "usage_count": c, "revenue": c*pr, "price": pr, "type": "Used"})
        b_list.sort(key=lambda x: x['revenue'], reverse=True)
        
        top_item = b_list[0]['item_name'] if b_list else "None"
        arpu = sum(x['revenue'] for x in b_list)

        final_metrics = { "total_plays": real_plays, "win_rate": win_rate, "arpu": arpu, "avg_balance": 0, "top_item": top_item }
        
        funnel = [
            {"event_type": "START", "count": metrics['start'], "revenue": 0}, # D√πng metrics['start'] cho ƒë√∫ng ph·ªÖu
            {"event_type": "WIN", "count": metrics['win'], "revenue": 0},
            {"event_type": "SPEND", "count": metrics['spend'], "revenue": metrics['rev']},
            {"event_type": "FAIL", "count": metrics['fail'], "revenue": 0}
        ]
        
        cost_arr = []
        if cost_dist['win_cost'] > 0: cost_arr.append({"name": "Cost to Win", "value": cost_dist['win_cost']})
        if cost_dist['fail_cost'] > 0: cost_arr.append({"name": "Wasted on Fail", "value": cost_dist['fail_cost']})

        # Pagination
        total_rec = len(filtered_rows)
        paged_data = filtered_rows[offset : offset + limit]
        proc_logs = []
        
        for r in paged_data:
            d = r['parsed']
            u = d.get('userID') or d.get('uuid') or "Guest"
            dt = []
            if d.get('coin_spent'): dt.append(f"üí∏ -{d['coin_spent']}")
            if d.get('coin_balance'): dt.append(f"üí∞ {d['coin_balance']}")
            
            for k,v in d.items():
                if ('booster' in k or 'revive' in k) and int(v) > 0:
                    clean = k.replace('booster_', '').replace('revive_', '') 
                    dt.append(f"‚ö° {clean} x{v}")
            
            proc_logs.append({
                "time": r['created_at'].strftime('%H:%M:%S %d/%m'),
                "user_id": str(u)[:15]+"..",
                "event_name": r['event_name'],
                "coin_spent": d.get('coin_spent', 0),
                "item_name": " | ".join(dt) if dt else "-"
            })

        return jsonify({
            "success": True,
            "metrics": final_metrics, "funnel": funnel, "booster_usage": b_list, 
            "cost_distribution": cost_arr,
            "logs": { "data": proc_logs, "pagination": { "current": page, "total_pages": (total_rec+limit-1)//limit, "total_records": total_rec } }
        })

    except Exception as e:
        print(f"Level Detail Error V97: {e}")
        return jsonify(default_resp)
    finally: conn.close()

@app.route("/dashboard/<int:app_id>/strategic", methods=['GET'])
def get_strategic_overview(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB error"}), 500

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        start_set = {"missionStart", "missionStart_Daily", "level_start", "level_loading_start", "missionStart_WeeklyQuestTutor"}
        fail_set = {"missionFail", "missionFail_Daily", "level_fail", "level_lose", "missionFail_WeeklyQuestTutor"}
        
        # L·∫§Y H·∫æT DATA (Ch·∫•p nh·∫≠n n·∫∑ng m·ªôt ch√∫t nh∆∞ng ch√≠nh x√°c cho c·∫£ 2 game)
        # V√¨ bi·ªÉu ƒë·ªì n√†y c·∫ßn t·ªïng h·ª£p m·ªçi level, ta kh√¥ng filter level c·ª• th·ªÉ ·ªü SQL
        where = "WHERE app_id = %s"; params = [app_id]
        if start_date: where += " AND created_at >= %s"; params.append(start_date + " 00:00:00")
        if end_date: where += " AND created_at <= %s"; params.append(end_date + " 23:59:59")
        
        cur.execute(f"SELECT event_name, event_json FROM event_logs {where}", tuple(params))
        rows = cur.fetchall()
        
        stats = {} 

        for r in rows:
            data = universal_flatten(r['event_json'])
            
            # T√¨m Level
            lvl_raw = data.get('levelID') or data.get('level_display') or data.get('missionID')
            if not lvl_raw: continue
            
            # L·∫•y s·ªë level
            digits = ''.join(filter(str.isdigit, str(lvl_raw)))
            if not digits: continue
            lvl_num = int(digits)
            if lvl_num > 2000: continue
            
            if lvl_num not in stats: stats[lvl_num] = {"plays": 0, "fails": 0, "rev": 0}
            
            evt = r['event_name']
            if evt in start_set: stats[lvl_num]['plays'] += 1
            elif evt in fail_set: stats[lvl_num]['fails'] += 1
            
            money = data.get('coin_spent') or data.get('coin_cost') or 0
            try: stats[lvl_num]['rev'] += float(money)
            except: pass

        chart = []
        for lvl, val in stats.items():
            # Ch·ªâ hi·ªán level c√≥ t∆∞∆°ng t√°c
            if val['plays'] > 0 or val['rev'] > 0 or val['fails'] > 0:
                fr = round((val['fails'] / val['plays']) * 100, 1) if val['plays'] > 0 else 0
                if fr > 100: fr = 100.0
                chart.append({
                    "name": f"Lv.{lvl}", "level_index": lvl,
                    "revenue": val['rev'], "fail_rate": fr, "plays": val['plays']
                })

        chart.sort(key=lambda x: x['level_index'])
        return jsonify({"success": True, "balance_chart": chart})

    except Exception as e:
        print(f"Strategic Error V97: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

# --- B·ªî SUNG API: X√ìA 1 D√íNG & STOP JOB ---
@app.route("/monitor/history/<int:id>", methods=['DELETE'])
def delete_single_history(id):
    conn = get_db()
    try:
        cur = conn.cursor()
        cur.execute("DELETE FROM job_history WHERE id = %s", (id,))
        conn.commit()
        return jsonify({"success": True, "msg": f"Deleted history #{id}"})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

@app.route("/etl/stop/<int:hist_id>", methods=['POST'])
def stop_etl_process(hist_id):
    # API n√†y d√πng ƒë·ªÉ ƒë√°nh d·∫•u job l√† Cancelled tr√™n Database
    # N√≥ c≈©ng c·ªë g·∫Øng reset tr·∫°ng th√°i b·∫≠n c·ªßa h·ªá th·ªëng n·∫øu c·∫ßn
    conn = get_db()
    try:
        # 1. K√≠ch ho·∫°t c·ªù d·ª´ng ƒë·ªÉ Worker ƒëang ch·∫°y t·ª± tho√°t
        if hist_id in JOB_STOP_EVENTS:
            print(f"üõë Sending STOP signal to Job #{hist_id}...")
            JOB_STOP_EVENTS[hist_id].set() # ƒê√°nh th·ª©c Worker ngay l·∫≠p t·ª©c
        cur = conn.cursor()
        # C·∫≠p nh·∫≠t DB
        cur.execute("""
            UPDATE job_history 
            SET status = 'Cancelled', end_time = NOW(), logs = logs || E'\n[USER MANUAL STOP]'
            WHERE id = %s AND status IN ('Running', 'Processing')
        """, (hist_id,))
        conn.commit()

        # 3. Reset h·ªá th·ªëng n·∫øu c·∫ßn
        if is_system_busy():
            set_system_busy(False)
            
        return jsonify({"success": True, "msg": f"Stop signal sent to Job #{hist_id}"})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

# --- [V41 FIX] API C·∫§U H√åNH ƒê·ªòNG (D√ôNG JSON DB) ---
@app.route("/apps/<int:app_id>/analytics-config", methods=['GET', 'POST'])
def handle_analytics_config(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB Connection Failed"}), 500
    
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # --- 1. L·∫§Y C·∫§U H√åNH (GET) ---
        if request.method == 'GET':
            # Query m·ªõi: Ch·ªâ l·∫•y c·ªôt config_json
            cur.execute("SELECT config_json FROM analytics_config WHERE app_id = %s", (app_id,))
            row = cur.fetchone()
            
            if row and row['config_json']:
                # Tr·∫£ v·ªÅ JSON chu·∫©n cho Frontend
                return jsonify(row['config_json'])
            else:
                # N·∫øu ch∆∞a c√≥ trong DB, tr·∫£ v·ªÅ config m·∫∑c ƒë·ªãnh t·ª´ h√†m get_app_config
                # (L∆∞u √Ω: B·∫°n ph·∫£i ƒë·∫£m b·∫£o h√†m get_app_config ·ªü ƒë·∫ßu file ƒë√£ s·ª≠a t√™n b·∫£ng th√†nh analytics_config nh√©)
                return jsonify(get_app_config(cur, app_id))

        # --- 2. L∆ØU C·∫§U H√åNH (POST) ---
        elif request.method == 'POST':
            new_config = request.json # Frontend g·ª≠i l√™n to√†n b·ªô c·ª•c JSON settings
            
            # Chuy·ªÉn Dict th√†nh String JSON ƒë·ªÉ l∆∞u v√†o DB
            config_str = json.dumps(new_config)

            # L∆∞u th·∫≥ng v√†o c·ªôt config_json (G·ªçn nh·∫π h∆°n logic c≈© r·∫•t nhi·ªÅu)
            cur.execute("""
                INSERT INTO analytics_config (app_id, config_json, updated_at)
                VALUES (%s, %s, NOW())
                ON CONFLICT (app_id) DO UPDATE SET 
                    config_json = EXCLUDED.config_json,
                    updated_at = NOW()
            """, (app_id, config_str))
            
            conn.commit()
            return jsonify({"success": True, "msg": "Configuration Saved Successfully (V40 JSON Mode)"})
            
    except Exception as e:
        print(f"‚ùå Analytics Config Error: {e}")
        conn.rollback() # Ch·ªëng k·∫πt transaction
        return jsonify({"success": False, "error": str(e)}), 500
    finally:
        conn.close()

# --- API CH·∫†Y ETL (T·ªîNG H·ª¢P D·ªÆ LI·ªÜU) ---
@app.route("/api/run-etl/<int:app_id>", methods=['POST'])
def trigger_etl_process(app_id):
    # Ch·∫°y trong thread ri√™ng ƒë·ªÉ kh√¥ng block server
    threading.Thread(target=run_etl_pipeline, args=(app_id,)).start()
    return jsonify({"status": "started", "message": "ETL process started in background"})

# --- API M·ªöI: TRA C·ª®U D·ªÆ LI·ªÜU TH√î (DATA EXPLORER) ---
@app.route("/events/search", methods=['GET'])
def search_events():
    try:
        app_id = request.args.get('app_id')
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 20))
        
        # C√°c b·ªô l·ªçc
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        event_name = request.args.get('event_name')
        keyword = request.args.get('keyword') # T√¨m UserID ho·∫∑c n·ªôi dung b·∫•t k·ª≥ trong JSON

        if not app_id:
            return jsonify({"success": False, "error": "Missing app_id"}), 400

        conn = get_db()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # 1. X√¢y d·ª±ng c√¢u WHERE ƒë·ªông
        where_clauses = ["app_id = %s"]
        params = [app_id]

        if start_date:
            where_clauses.append("created_at >= %s")
            params.append(start_date + " 00:00:00")
        
        if end_date:
            where_clauses.append("created_at <= %s")
            params.append(end_date + " 23:59:59")

        if event_name and event_name.strip():
            where_clauses.append("event_name = %s")
            params.append(event_name)

        if keyword and keyword.strip():
            # K·ªπ thu·∫≠t t√¨m ki·∫øm trong JSON (Chuy·ªÉn JSON th√†nh Text ƒë·ªÉ t√¨m)
            where_clauses.append("event_json::text ILIKE %s")
            params.append(f"%{keyword}%")

        full_where = " WHERE " + " AND ".join(where_clauses)

        # 2. ƒê·∫øm t·ªïng s·ªë d√≤ng (ƒë·ªÉ l√†m ph√¢n trang 1/100...)
        count_query = f"SELECT COUNT(*) as total FROM event_logs {full_where}"
        cursor.execute(count_query, tuple(params))
        total_records = cursor.fetchone()['total']
        total_pages = (total_records + limit - 1) // limit

        # 3. L·∫•y d·ªØ li·ªáu ph√¢n trang
        offset = (page - 1) * limit
        data_query = f"""
            SELECT 
                id, 
                event_name, 
                to_char(created_at, 'YYYY-MM-DD HH24:MI:SS') as created_at,
                event_json -- L·∫•y nguy√™n c·ª•c JSON v·ªÅ ƒë·ªÉ Frontend hi·ªÉn th·ªã ƒë·∫πp
            FROM event_logs 
            {full_where}
            ORDER BY created_at DESC
            LIMIT %s OFFSET %s
        """
        # Th√™m limit/offset v√†o params
        params.extend([limit, offset])
        
        cursor.execute(data_query, tuple(params))
        rows = cursor.fetchall()

        # 4. Tr√≠ch xu·∫•t s∆° b·ªô User ID ƒë·ªÉ hi·ªÉn th·ªã ra ngo√†i b·∫£ng (cho ti·ªán nh√¨n)
        for row in rows:
            try:
                # Parse JSON string th√†nh Dict
                import json
                raw = row['event_json']
                # X·ª≠ l√Ω double-encoded n·∫øu c√≥
                if isinstance(raw, str):
                    parsed = json.loads(raw)
                    # N·∫øu b√™n trong l·∫°i c√≥ key 'event_json' d·∫°ng string
                    if isinstance(parsed, dict) and 'event_json' in parsed and isinstance(parsed['event_json'], str):
                        inner = json.loads(parsed['event_json'])
                        parsed.update(inner)
                    row['event_json'] = parsed # G√°n l·∫°i object ƒë√£ s·∫°ch
                
                # --- [LOGIC M·ªöI] T·∫†O C·ªòT KEY INFO ---
                # Thay v√¨ l·∫•y UserID, ta l·∫•y th√¥ng tin ng·ªØ c·∫£nh quan tr·ªçng h∆°n
                data = row['event_json']
                info_parts = []
                
                # 1. N·∫øu c√≥ th√¥ng tin Level/Mission -> L·∫•y ngay
                if 'levelID' in data: info_parts.append(f"Lv.{data['levelID']}")
                if 'missionID' in data: info_parts.append(f"Ms.{data['missionID']}")
                
                # 2. N·∫øu c√≥ th√¥ng tin Ti·ªÅn/Gi√° -> L·∫•y ngay
                if 'coin_cost' in data: info_parts.append(f"-{data['coin_cost']} Coin")
                if 'coin_price' in data: info_parts.append(f"-{data['coin_price']} Coin")
                if 'revenue' in data: info_parts.append(f"+{data['revenue']} USD")
                
                # 3. N·∫øu c√≥ th√¥ng tin Item/Booster
                if 'item_name' in data: info_parts.append(data['item_name'])
                
                # G√°n v√†o bi·∫øn m·ªõi ƒë·ªÉ tr·∫£ v·ªÅ Frontend
                row['key_info'] = " | ".join(info_parts) if info_parts else "..."
            except:
                row['key_info'] = '-'
                row['event_json'] = {}

        return jsonify({
            "success": True,
            "data": rows,
            "pagination": {
                "current_page": page,
                "total_pages": total_pages,
                "total_records": total_records,
                "limit": limit
            }
        })

    except Exception as e:
        print(f"Search Error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally:
        conn.close()

if __name__ == '__main__':
    # --- [V105] CLEANUP ZOMBIE JOBS ON STARTUP ---
    # T·ª± ƒë·ªông ƒë√°nh d·∫•u Failed cho c√°c job ƒëang treo do server restart
    try:
        conn = get_db()
        if conn:
            cur = conn.cursor()
            cur.execute("""
                UPDATE job_history 
                SET status = 'Failed', end_time = NOW(), logs = logs || E'\n[System Restart] Job died unexpectedly.'
                WHERE status IN ('Running', 'Processing')
            """)
            killed_count = cur.rowcount
            if killed_count > 0:
                print(f"üßπ Cleanup: Killed {killed_count} jobs from previous session.")
            conn.commit()
            conn.close()
    except Exception as e:
        print(f"‚ö†Ô∏è Cleanup Warning: {e}")

    # Kh·ªüi ƒë·ªông c√°c lu·ªìng ng·∫ßm
    t1 = threading.Thread(target=run_scheduler_loop)
    t1.daemon = True
    t1.start()

    t2 = threading.Thread(target=run_worker_loop)
    t2.daemon = True
    t2.start()

    print("üöÄ SYSTEM READY: Smart Scheduler & Worker Threads started...")
    app.run(host='0.0.0.0', port=8080, debug=True, use_reloader=False)