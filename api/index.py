from etl_processor import run_etl_pipeline
from flask import Flask, jsonify, request
from flask_cors import CORS
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime, timedelta
import time
import requests
import threading
import random
import os                       
from dotenv import load_dotenv
import re

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

load_dotenv()
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# --- 3. S·ª¨A C·∫§U H√åNH DATABASE (L·∫•y t·ª´ .env) ---
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_NAME = os.getenv("DB_NAME", "p1_gamedata")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASS = os.getenv("DB_PASS") 

# Ki·ªÉm tra an to√†n: N·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c pass th√¨ b√°o l·ªói
if not DB_PASS:
    print("‚ö†Ô∏è  C·∫¢NH B√ÅO: Ch∆∞a t√¨m th·∫•y DB_PASS trong file .env")

# --- C·∫§U H√åNH H·ªÜ TH·ªêNG ---
MAX_RETRIES = 18       # S·ªë l·∫ßn th·ª≠ l·∫°i t·ªëi ƒëa cho Auto Worker

# --- TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG TO√ÄN C·ª§C ---
SYSTEM_STATE = {
    "is_busy": False,          
    "current_app_id": None,    
    "current_run_type": None   
}
SYSTEM_LOCK = threading.Lock() 
JOB_STOP_EVENTS = {}

def universal_flatten(raw_input):
    """
    H√†m V95: Khoan s√¢u v√†o m·ªçi ng√≥c ng√°ch c·ªßa JSON (H·ªó tr·ª£ n l·ªõp l·ªìng nhau).
    Tr·∫£ v·ªÅ: M·ªôt dictionary ph·∫≥ng ch·ª©a t·∫•t c·∫£ th√¥ng tin.
    """
    if not raw_input: return {}
    
    data = {}
    # L·ªõp 1: Parse t·ª´ DB (th∆∞·ªùng l√† string ho·∫∑c dict)
    try:
        if isinstance(raw_input, str):
            data = json.loads(raw_input)
        elif isinstance(raw_input, dict):
            data = raw_input.copy()
    except: return {}

    # L·ªõp 2: Ki·ªÉm tra c√°c key ch·ª©a JSON l·ªìng nhau th∆∞·ªùng g·∫∑p
    # Game 2 th∆∞·ªùng nh√©t d·ªØ li·ªáu v√†o key 'event_json' ho·∫∑c 'params'
    nested_keys = ['event_json', 'params', 'data', 'attributes']
    
    for key in nested_keys:
        if key in data and isinstance(data[key], str):
            try:
                inner = json.loads(data[key])
                if isinstance(inner, dict):
                    data.update(inner) # G·ªôp d·ªØ li·ªáu con ra ngo√†i
            except: pass
            
    # L·ªõp 3: X·ª≠ l√Ω Double Encode (Tr∆∞·ªùng h·ª£p chu·ªói b·ªã m√£ h√≥a 2 l·∫ßn)
    if isinstance(data, str): 
        try: data = json.loads(data)
        except: pass
        
    return data if isinstance(data, dict) else {}

def get_app_config(cur, app_id):
    """
    H√†m l·∫•y c·∫•u h√¨nh ƒë·ªông t·ª´ Database.
    N·∫øu kh√¥ng c√≥, tr·∫£ v·ªÅ c·∫•u h√¨nh m·∫∑c ƒë·ªãnh (Fallback) ƒë·ªÉ tr√°nh l·ªói.
    """
    try:
        cur.execute("SELECT config_json FROM analytics_config WHERE app_id = %s", (app_id,))
        row = cur.fetchone()
        if row and row['config_json']:
            return row['config_json']
    except Exception as e:
        print(f"Config Warning: {e}")
        # Quan tr·ªçng: N·∫øu query l·ªói, ph·∫£i rollback ƒë·ªÉ kh√¥ng k·∫πt transaction sau n√†y
        if cur.connection:
            cur.connection.rollback()

    # C·∫•u h√¨nh M·∫∑c ƒë·ªãnh (Fallback) n·∫øu ch∆∞a setup DB
    return {
        "events": {
            "start": ["missionStart", "missionStart_Daily", "missionStart_WeeklyQuestTutor"],
            "win": ["missionComplete", "missionComplete_Daily", "missionComplete_WeeklyQuestTutor"],
            "progress": ["missionProgress"],
            "fail": ["missionFail", "missionFail_Daily", "missionFail_WeeklyQuestTutor"],
            "transaction": {
                "real_currency": ["iapSuccess", "firstIAP"], # <--- ƒê√£ th√™m d·∫•u ph·∫©y
                "virtual_currency_exclude": ["iapSuccess", "firstIAP", "iapPurchase", "priceSpendLevel"], # <--- ƒê√£ th√™m d·∫•u ph·∫©y
                "offer_and_reward": ["FirstReward", "adsRewardComplete", "iapOfferGet", "dailyReward"]
            }
        },
        "boosters": [ # <--- S·ª≠a ngo·∫∑c nh·ªçn { th√†nh ngo·∫∑c vu√¥ng [
            {"key": "booster_Hammer", "name": "Hammer üî®", "type": "booster"},
            {"key": "booster_Magnet", "name": "Magnet üß≤", "type": "booster"},
            {"key": "booster_Add", "name": "Add Moves ‚ûï", "type": "booster"},
            {"key": "booster_Unlock", "name": "Unlock üîì", "type": "booster"},
            {"key": "booster_Clear", "name": "Clear üßπ", "type": "booster"},
            {"key": "revive_boosterClear", "name": "Revive üíñ", "type": "revive"}
        ], # <--- S·ª≠a ngo·∫∑c nh·ªçn } th√†nh ngo·∫∑c vu√¥ng [
        "currency": {
            "real": ["VND", "USD", "‚Ç´", "$"], # <--- ƒê√£ th√™m d·∫•u ph·∫©y
            "virtual": ["Coin"]
        }
    }

def smart_parse_json(raw_input):
    """
    H√†m th√¥ng minh ƒë·ªÉ x·ª≠ l√Ω tr∆∞·ªùng h·ª£p JSON b·ªã l·ªìng 2 l·ªõp.
    V√≠ d·ª•: "{\"event_json\": \"{\\\"levelID\\\": 1...}\"}"
    """
    if not raw_input: 
        return {}
    
    try:
        # L·ªõp 1: N·∫øu l√† string th√¨ parse ra dict, n·∫øu l√† dict r·ªìi th√¨ gi·ªØ nguy√™n
        parsed_data = json.loads(raw_input) if isinstance(raw_input, str) else raw_input
        
        # L·ªõp 2: Ki·ªÉm tra xem b√™n trong c√≥ key 'event_json' ch·ª©a string JSON n·ªØa kh√¥ng (L·ªói double encode)
        if isinstance(parsed_data, dict) and 'event_json' in parsed_data:
            inner_value = parsed_data['event_json']
            if isinstance(inner_value, str):
                try:
                    inner_json = json.loads(inner_value)
                    # G·ªôp d·ªØ li·ªáu b√™n trong ra ngo√†i (Flatten)
                    parsed_data.update(inner_json)
                except:
                    pass # N·∫øu kh√¥ng parse ƒë∆∞·ª£c l·ªõp trong th√¨ th√¥i
                    
        return parsed_data
    except Exception:
        return {}

def get_db():
    try:
        conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS)
        return conn
    except Exception as e:
        print("‚ùå L·ªñI K·∫æT N·ªêI DB:", e)
        return None

# --- H√ÄM QU·∫¢N L√ù TR·∫†NG TH√ÅI B·∫¨N/R·∫¢NH ---
def set_system_busy(busy, app_id=None, run_type=None):
    with SYSTEM_LOCK:
        SYSTEM_STATE["is_busy"] = busy
        SYSTEM_STATE["current_app_id"] = app_id
        SYSTEM_STATE["current_run_type"] = run_type

def is_system_busy():
    with SYSTEM_LOCK:
        return SYSTEM_STATE["is_busy"]

# ==========================================
# PH·∫¶N 1: CORE FUNCTIONS (T·∫†O JOB & C·∫¨P NH·∫¨T)
# ==========================================

def create_etl_job(app_id, date_since, date_until):
    conn = get_db()
    if not conn: return
    cur = conn.cursor()
    try:
        # Check tr√πng: N·∫øu ƒë√£ c√≥ job ƒëang ch·ªù/ch·∫°y c√πng khung gi·ªù th√¨ th√¥i
        cur.execute("""
            SELECT id FROM etl_jobs 
            WHERE app_id = %s AND date_since = %s AND status IN ('pending', 'processing')
        """, (app_id, date_since))
        if cur.fetchone(): 
            return # ƒê√£ c√≥ job r·ªìi, kh√¥ng t·∫°o th√™m

        cur.execute("""
            INSERT INTO etl_jobs (app_id, date_since, date_until, status, retry_count, message, created_at)
            VALUES (%s, %s, %s, 'pending', 0, 'Scheduled Auto', NOW())
        """, (app_id, date_since, date_until))
        conn.commit()
        print(f"üé´ Auto: ƒê√£ t·∫°o v√© Job cho App {app_id}")
    except Exception as e:
        print(f"‚ùå Auto Error: {e}")
    finally:
        cur.close()
        conn.close()

def update_job_status(job_id, status, message=None, inc_retry=False):
    conn = get_db()
    if not conn: return
    cur = conn.cursor()
    try:
        sql = "UPDATE etl_jobs SET status = %s, updated_at = NOW(), message = %s"
        if inc_retry: sql += ", retry_count = retry_count + 1"
        sql += " WHERE id = %s"
        cur.execute(sql, (status, message, job_id))
        conn.commit()
    finally:
        cur.close()
        conn.close()

# ==========================================
# PH·∫¶N 2: WORKER TH√îNG MINH (ƒê√É S·ª¨A L·ªñI TIME & INSERT TR∆Ø·ªöC)
# ==========================================
# --- H√ÄM PH·ª§ TR·ª¢: GHI LOG V√ÄO DB ---
def append_log_to_db(hist_id, new_log_line):
    """N·ªëi th√™m log v√†o d√≤ng l·ªãch s·ª≠ ƒëang ch·∫°y"""   
    if not hist_id: return
    try:
        conn = get_db()
        cur = conn.cursor()
        # D√πng to√°n t·ª≠ || ƒë·ªÉ n·ªëi chu·ªói trong PostgreSQL
        timestamp = datetime.now().strftime('%H:%M:%S')
        log_entry = f"\n[{timestamp}] {new_log_line}"
        
        cur.execute("""
            UPDATE job_history 
            SET logs = COALESCE(logs, '') || %s, updated_at = NOW()
            WHERE id = %s
        """, (log_entry, hist_id))
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"‚ùå Error appending log: {e}")

def transform_events_to_level_analytics(app_id, events):
    """
    [UPDATED] Transform missionStart / missionComplete / missionFail
    S·ª≠ d·ª•ng smart_parse_json ƒë·ªÉ x·ª≠ l√Ω l·ªói l·ªìng JSON.
    """
    if not events:
        return

    conn = get_db()
    cur = conn.cursor()

    # 1. Gom event theo (user_id, level_id)
    sessions = {}

    for e in events:
        try:
            event_name = e.get("event_name")
            raw_json = smart_parse_json(e.get("event_json"))
            level_id = (raw_json.get("levelID") or 
                        raw_json.get("missionID") or 
                        raw_json.get("level_display") or 
                        raw_json.get("level_display_origin"))
            user_id = (raw_json.get("userID") or "Guest" or
                       raw_json.get("uuid"))
            # N·∫øu v·∫´n kh√¥ng l·∫•y ƒë∆∞·ª£c level_id, b·ªè qua event n√†y
            if not level_id: continue
            session_key = f"{user_id}_{level_id}"
            # Timestamp x·ª≠ l√Ω an to√†n
            try:
                ts_val = int(e.get("event_timestamp"))
                ts = datetime.fromtimestamp(ts_val)
            except:
                ts = datetime.now()

            if session_key not in sessions:
                sessions[session_key] = {
                    "app_id": app_id,
                    "user_id": user_id,
                    "level_id": level_id,
                    "start_time": None,
                    "end_time": None,
                    "status": "DROP",
                    "total_cost": 0
                }

            s = sessions[session_key]

            # Logic t√≠nh to√°n gi·ªØ nguy√™n, ch·ªâ ƒë·∫£m b·∫£o raw_json ƒë√£ s·∫°ch
            if event_name == "missionStart":
                s["start_time"] = ts

            elif event_name == "missionComplete":
                s["end_time"] = ts
                s["status"] = "WIN"
                for k, v in raw_json.items():
                    if k.startswith("booster_") or k.startswith("revive_"):
                        try: s["total_cost"] += int(v)
                        except: pass

            elif event_name == "missionFail":
                s["end_time"] = ts
                s["status"] = "FAIL"
                for k, v in raw_json.items():
                    if k.startswith("booster_") or k.startswith("revive_"):
                        try: s["total_cost"] += int(v)
                        except: pass

        except Exception as ex:
            print(f"Transform error skipping row: {ex}")

    # 2. Insert v√†o level_analytics
    for s in sessions.values():
        start_time = s["start_time"]
        end_time = s["end_time"]
        duration = 0
        if start_time and end_time:
            duration = int((end_time - start_time).total_seconds())

        # Ch·ªâ insert n·∫øu c√≥ d·ªØ li·ªáu h·ª£p l·ªá (Tr√°nh r√°c)
        try:
            cur.execute("""
                INSERT INTO level_analytics
                (app_id, session_id, user_id, level_name, status, duration, start_time, total_cost, created_at)
                VALUES (%s,%s,%s,%s,%s,%s,%s,%s,NOW())
            """, (
                s["app_id"],
                f"{s['user_id']}_{s['level_id']}",
                s["user_id"],
                f"Level {s['level_id']}", # Format t√™n Level ƒë·∫πp h∆°n
                s["status"],
                duration,
                start_time,
                s["total_cost"]
            ))
        except Exception as insert_err:
            print(f"Insert Analytics Error: {insert_err}")

    conn.commit()
    conn.close()

def worker_process_jobs():
    if is_system_busy(): return

    conn = get_db()
    if not conn: return
    
    # 1. L·∫•y Job ƒëang ch·ªù
    cur = conn.cursor(cursor_factory=RealDictCursor)
    cur.execute("""
        SELECT * FROM etl_jobs 
        WHERE status IN ('pending', 'processing') 
        ORDER BY created_at ASC LIMIT 1
    """)
    job = cur.fetchone()
    cur.close() 
    conn.close()

    if not job: return

    job_id = job['id']
    app_id = job['app_id']
    retry_count = job['retry_count']

    # ================= [B·∫ÆT ƒê·∫¶U ƒêO·∫†N C·∫¶N TH√äM] =================
    # LOGIC: N·∫øu th·∫•y l√† Retry, t·ª± ƒë·ªông l·ªôi ng∆∞·ª£c d√≤ng t√¨m ng√†y c≈©
    # (Ghi ƒë√® l·∫°i date_since/date_until m√† API ƒë√£ t√≠nh sai)
    if job.get('run_type') == 'retry' and job.get('retry_job_id'):
        try:
            print(f"üïµÔ∏è‚Äç‚ôÇÔ∏è Worker ph√°t hi·ªán Retry cho Job #{job['retry_job_id']}. ƒêang t√≠nh l·∫°i ng√†y...")
            conn_fix = get_db()
            cur_fix = conn_fix.cursor(cursor_factory=RealDictCursor)
            
            # L·∫•y gi·ªù ch·∫°y c·ªßa Job qu√° kh·ª©
            cur_fix.execute("SELECT start_time FROM job_history WHERE id = %s", (job['retry_job_id'],))
            old_job = cur_fix.fetchone()
            cur_fix.close()
            conn_fix.close()

            if old_job and old_job['start_time']:
                # T√≠nh l·∫°i c·ª≠a s·ªï th·ªùi gian (gi·ªëng logic chu k·ª≥ 1 ti·∫øng)
                fix_target = old_job['start_time']
                fix_from = fix_target - timedelta(minutes=65)
                
                # C·∫¨P NH·∫¨T L·∫†I D·ªÆ LI·ªÜU TRONG B·ªò NH·ªö
                job['date_until'] = fix_target.strftime('%Y-%m-%d %H:%M:%S')
                job['date_since'] = fix_from.strftime('%Y-%m-%d %H:%M:%S')
                print(f"‚úÖ ƒê√£ ƒëi·ªÅu ch·ªânh th·ªùi gian v·ªÅ qu√° kh·ª©: {job['date_since']} -> {job['date_until']}")
        except Exception as e_fix:
            print(f"‚ö†Ô∏è L·ªói khi t√≠nh l·∫°i ng√†y Retry: {e_fix}")
    # ================= [K·∫æT TH√öC ƒêO·∫†N C·∫¶N TH√äM] =================

    # 2. X·ª≠ l√Ω qu√° h·∫°n Retry
    if retry_count >= MAX_RETRIES:
        print(f"üíÄ Job #{job_id} MAX RETRIES. Failed.")
        update_job_status(job_id, 'failed', f"Timeout: {retry_count} retries.")
        # C·∫≠p nh·∫≠t history l·∫ßn cu·ªëi n·∫øu c√≥
        conn = get_db()
        cur = conn.cursor()
        cur.execute("UPDATE job_history SET status='Failed', end_time=NOW() WHERE app_id=%s AND status IN ('Running','Processing')", (app_id,))
        conn.commit()
        conn.close()
        return 

    set_system_busy(True, app_id, 'auto')

    # 3. --- [QUAN TR·ªåNG] T√åM HO·∫∂C T·∫†O HISTORY ---
    # M·ª•c ƒë√≠ch: ƒê·ªÉ c√°c l·∫ßn Retry sau v·∫´n n·ªëi v√†o log c·ªßa l·∫ßn ƒë·∫ßu ti√™n
    hist_id = None
    try:
        conn = get_db()
        cur = conn.cursor()
        # T√¨m history ƒëang ch·∫°y d·ªü (Processing) c·ªßa App n√†y
        cur.execute("""
            SELECT id FROM job_history 
            WHERE app_id = %s AND status IN ('Running', 'Processing') 
            ORDER BY start_time DESC LIMIT 1
        """, (app_id,))
        row = cur.fetchone()
        
        if row:
            hist_id = row[0] # D√πng l·∫°i ID c≈© ƒë·ªÉ n·ªëi log
        else:
            # T·∫°o m·ªõi n·∫øu ch∆∞a c√≥ (L·∫ßn ch·∫°y ƒë·∫ßu ti√™n)
            cur.execute("""
                INSERT INTO job_history (app_id, start_time, status, run_type, logs, total_events)
                VALUES (%s, NOW(), 'Processing', 'schedule', '', 0)
                RETURNING id
            """, (app_id,))
            hist_id = cur.fetchone()[0]
        conn.commit()
        conn.close()
    except Exception as e:
        print(f"‚ùå History Init Error: {e}")

    try:
        # H√†m log c·ª•c b·ªô: v·ª´a in ra m√†n h√¨nh, v·ª´a ƒë·∫©y v√†o DB ngay l·∫≠p t·ª©c
        def log(msg):
            print(msg)
            append_log_to_db(hist_id, msg)

        log(f"‚ñ∂Ô∏è Worker picking up Job #{job_id} (Retry: {retry_count}/{MAX_RETRIES})")

        try:
            # 1. Parse chu·ªói gi·ªù UTC t·ª´ Database ra
            utc_start = datetime.strptime(str(job['date_since']), '%Y-%m-%d %H:%M:%S')
            utc_end = datetime.strptime(str(job['date_until']), '%Y-%m-%d %H:%M:%S')
            
            # 2. C·ªông th√™m 7 ti·∫øng ƒë·ªÉ ra gi·ªù Vi·ªát Nam
            vn_start = utc_start + timedelta(hours=7)
            vn_end = utc_end + timedelta(hours=7)
            
            # 3. Format l·∫°i cho ƒë·∫πp (Gi·ªëng Terminal)
            log(f" üïí Scanning Window: VN[{vn_start.strftime('%H:%M')} - {vn_end.strftime('%H:%M')}] (UTC: {utc_start.strftime('%H:%M')} - {utc_end.strftime('%H:%M')})")
        except:
            # Fallback: N·∫øu l·ªói format th√¨ in nguy√™n g·ªëc
            log(f" üïí Scanning Window: {job['date_since']} -> {job['date_until']}")

        # L·∫•y th√¥ng tin App
        conn = get_db()
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT * FROM apps WHERE id = %s", (app_id,))
        app_info = cur.fetchone()
        cur.close()
        conn.close()

        if not app_info:
            log("‚ùå Error: App not found or deleted.")
            update_job_status(job_id, 'failed', 'App deleted')
            return
        
        clean_app_id = str(app_info['app_id']).strip()
        clean_token = str(app_info['api_token']).strip()
        # G·ªçi API AppMetrica
        url = "https://api.appmetrica.yandex.com/logs/v1/export/events.json"
        params = {
            "application_id": clean_app_id,
            "date_since": str(job['date_since']), 
            "date_until": str(job['date_until']),
            "fields": "event_name,event_timestamp,event_json",
            "limit": 1000000 
        }
        headers = {"Authorization": f"OAuth {clean_token}"}

        log(f"  üì° Connecting to AppMetrica...")
        response = requests.get(url, params=params, headers=headers, stream=True, timeout=600)
        
        if response.status_code == 200:
            log("  ‚úÖ Connection Established (200 OK). Downloading...")
            
            conn = get_db()
            cur = conn.cursor()
            
            data = response.json()
            events = data.get('data', [])
            event_count = len(events)
            
            # Insert d·ªØ li·ªáu
            for event in events:
                evt_name = event.get('event_name', 'unknown')
                evt_json = json.dumps(event)
                try: ts = datetime.fromtimestamp(int(event.get('event_timestamp')))
                except: ts = datetime.now()
                
                cur.execute("""
                    INSERT INTO event_logs (app_id, event_name, event_json, count, created_at) 
                    VALUES (%s, %s, %s, 1, %s)
                """, (app_id, evt_name, evt_json, ts))
            try:
                transform_events_to_level_analytics(app_id, events)
                # L·ªñI C≈®: logs(...) -> ƒê·ªîI TH√ÄNH log(...)
                log(f"ETL transform completed for {len(events)} events") 
            except Exception as e:
                # L·ªñI C≈®: logs(...) -> ƒê·ªîI TH√ÄNH log(...)
                log(f"ETL transform error: {str(e)}")       
            # --- C·∫¨P NH·∫¨T TR·∫†NG TH√ÅI CU·ªêI C√ôNG ---
            # Status: Success, c·∫≠p nh·∫≠t End Time -> Duration s·∫Ω t√≠nh ƒë√∫ng
            if hist_id:
                cur.execute("""
                    UPDATE job_history 
                    SET end_time = NOW(), status = 'Success', total_events = %s, success_count = %s
                    WHERE id = %s
                """, (event_count, event_count, hist_id))
            
            conn.commit()
            conn.close()
            
            update_job_status(job_id, 'completed', f"Done. {event_count} events.")
            log(f"  üéâ Job Completed. Imported: {event_count} events.")

        elif response.status_code == 202:
            wait_time=180
            log(f"  ‚è≥ HTTP 202: Data not ready. Waiting...")
            # Kh√¥ng ƒë√≥ng History, ƒë·ªÉ tr·∫°ng th√°i Processing ƒë·ªÉ l·∫ßn sau n·ªëi log ti·∫øp
            update_job_status(job_id, 'processing', 'Waiting for AppMetrica (202)...', inc_retry=True)
            time.sleep(wait_time) 
        
        elif response.status_code == 429:
            # 1. In to√†n b·ªô n·ªôi dung l·ªói ra ƒë·ªÉ xem n√≥ b·∫Øt ch·ªù bao l√¢u (th∆∞·ªùng n√≥ vi·∫øt trong n√†y)
            error_body = response.text
            log(f" ‚õî B·ªä CH·∫∂N (429)! N·ªôi dung t·ª´ Server: {error_body}")
            
            # 2. ƒê√°nh d·∫•u Job l√† FAILED ngay l·∫≠p t·ª©c (ƒê·ªÉ Worker kh√¥ng b·ªã k·∫πt)
            update_job_status(job_id, 'failed', f"Rate Limit 429. Server said: {error_body[:100]}...")
            
            # 3. ƒê√≥ng d√≤ng l·ªãch s·ª≠ ch·∫°y
            conn = get_db()
            cur = conn.cursor()
            cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
            conn.commit()
            conn.close()
            
            # 4. QUAN TR·ªåNG: Return lu√¥n ƒë·ªÉ tho√°t kh·ªèi h√†m, gi·∫£i ph√≥ng Worker
            log(" üõë D·ª´ng Job hi·ªán t·∫°i ƒë·ªÉ b·∫£o to√†n l·ª±c l∆∞·ª£ng. Vui l√≤ng ki·ªÉm tra log v√† th·ª≠ l·∫°i sau.")
            return

        else:
            log(f"  ‚ùå HTTP Error {response.status_code}")
            update_job_status(job_id, 'failed', f"HTTP {response.status_code}")
            # ƒê√≥ng History v√¨ l·ªói
            conn = get_db()
            cur = conn.cursor()
            cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
            conn.commit()
            conn.close()

    except Exception as e:
        log(f"  ‚ùå Exception: {str(e)}")
        update_job_status(job_id, 'failed', str(e))
        conn = get_db()
        cur = conn.cursor()
        cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
        conn.commit()
        conn.close()
    
    finally:
        set_system_busy(False)

def run_worker_loop():
    print("üöÄ Worker Loop Started...")
    while True:
        try:
            worker_process_jobs()
        except Exception as e:
            print(f"‚ùå Worker Loop Error: {e}")
        time.sleep(20)

# ==========================================
# PH·∫¶N 3: SCHEDULER TH√îNG MINH 
# ==========================================
def run_scheduler_loop():
    print("üöÄ Smart Scheduler Started (Anchor Time & Skip Logic)...")
    while True:
        try:
            now = datetime.now()
            conn = get_db()
            cur = conn.cursor(cursor_factory=RealDictCursor)
            cur.execute("SELECT * FROM apps WHERE is_active = true") 
            apps = cur.fetchall()

            for app in apps:
                app_id = app['id']
                interval_minutes = app.get('interval_minutes', 60) 
                if not interval_minutes: interval_minutes = 60

                sch_time_str = app.get('schedule_time', '00:00')
                try:
                    h, m = map(int, sch_time_str.split(':'))
                    anchor_time = now.replace(hour=h, minute=m, second=0, microsecond=0)
                except:
                    anchor_time = now.replace(hour=0, minute=0, second=0, microsecond=0)
                
                if anchor_time > now:
                    anchor_time = anchor_time - timedelta(days=1)
                
                diff = now - anchor_time
                cycles_passed = int(diff.total_seconds() // (interval_minutes * 60))
                
                expected_run_time = anchor_time + timedelta(minutes=cycles_passed * interval_minutes)
                
                time_since_expected = (now - expected_run_time).total_seconds()
                is_time_to_run = 0 <= time_since_expected < 65

                if is_time_to_run:
                    cur.execute("SELECT count(*) as count FROM etl_jobs WHERE app_id = %s AND created_at > %s", 
                                (app_id, now - timedelta(minutes=2)))
                    
                    if cur.fetchone()['count'] == 0:
                        print(f"‚è∞ Triggering Schedule for App #{app_id} at {now.strftime('%H:%M:%S')}")
                        
                        if is_system_busy():
                            print(f"‚ö†Ô∏è SKIPPING Auto Schedule for App #{app_id} - System is BUSY")
                            cur.execute("""
                                INSERT INTO job_history (app_id, start_time, status, run_type, logs) 
                                VALUES (%s, NOW(), 'Skipped', 'schedule', 'Skipped due to System Busy (Conflict)')
                            """, (app_id,))
                            conn.commit()
                        else:
                            delay_minutes = 90
                            end_time_vn = now - timedelta(minutes=delay_minutes)
                            start_time_vn = end_time_vn - timedelta(minutes=interval_minutes)
                            
                            end_time_utc = end_time_vn - timedelta(hours=7)
                            start_time_utc = start_time_vn - timedelta(hours=7)
                            
                            date_until = end_time_utc.strftime('%Y-%m-%d %H:%M:%S')
                            date_since = start_time_utc.strftime('%Y-%m-%d %H:%M:%S')

                            print(f"  üé´ Creating Job: VN[{start_time_vn.strftime('%H:%M')} - {end_time_vn.strftime('%H:%M')}] -> UTC[{date_since} - {date_until}]")
                            create_etl_job(app_id, date_since, date_until)

            cur.close()
            conn.close()
        except Exception as e:
            print(f"‚ùå Scheduler Error: {e}")
        
        time.sleep(60)

# PH·∫¶N 4: LOGIC CH·∫†Y TAY (MANUAL) - [FIXED RETRY LOGIC]
def perform_manual_etl(app_id, run_type='manual', is_demo=False, retry_job_id=None):
    if is_system_busy():
        print(f"‚ùå System BUSY. Skip run for App {app_id}.")
        return

    set_system_busy(True, app_id, run_type)
    hist_id = None

    try:
        conn = get_db()
        if not conn: return
        
        # 1. T·∫†O RECORD HISTORY
        msg_start = f"üöÄ Starting {run_type.upper()} run..."
        if retry_job_id: msg_start += f" (Retry of Job #{retry_job_id})"
        
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO job_history (app_id, start_time, status, run_type, logs, total_events) 
            VALUES (%s, NOW(), 'Running', %s, %s, 0) 
            RETURNING id
        """, (app_id, run_type, f"[{datetime.now().strftime('%H:%M:%S')}] {msg_start}"))
        hist_id = cur.fetchone()[0]
        conn.commit()
        
        stop_event = threading.Event()
        JOB_STOP_EVENTS[hist_id] = stop_event

        def log(msg):
            print(msg)
            append_log_to_db(hist_id, msg)

        # 2. L·∫§Y C·∫§U H√åNH APP
        cur = conn.cursor(cursor_factory=RealDictCursor)
        cur.execute("SELECT * FROM apps WHERE id=%s", (app_id,))
        app = cur.fetchone()
        
        if not app:
            log("‚ùå Error: App ID not found.")
            cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
            conn.commit()
            return 

        # 3. C·∫§U H√åNH TH·ªúI GIAN
        date_since = None
        date_until = None
        
        # --- CASE 1: STRICT RETRY (Logic V85 - V√©t c·∫°n Regex) ---
        if run_type == 'retry' and retry_job_id:
            cur.execute("SELECT logs, start_time FROM job_history WHERE id = %s", (retry_job_id,))
            old_row = cur.fetchone()
            
            if old_row and old_row['logs']:
                logs = old_row['logs']
                
                # A. T√¨m 2 chu·ªói ng√†y gi·ªù ƒë·∫ßy ƒë·ªß (YYYY-MM-DD HH:MM:SS)
                # B·∫•t k·ªÉ n√≥ n·∫±m trong d·∫•u [], (), hay sau d·∫•u :
                full_timestamps = re.findall(r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})", logs)
                
                if len(full_timestamps) >= 2:
                    # L·∫•y 2 m·ªëc cu·ªëi c√πng t√¨m ƒë∆∞·ª£c (Th∆∞·ªùng l√† Scanning Window)
                    date_since = full_timestamps[-2]
                    date_until = full_timestamps[-1]
                    log(f"üîô RETRY V85: Found exact window: {date_since} -> {date_until}")
                
                # B. N·∫øu kh√¥ng c√≥ ng√†y, t√¨m 2 chu·ªói gi·ªù (HH:MM)
                elif old_row['start_time']:
                    short_times = re.findall(r"(\d{2}:\d{2})", logs)
                    if len(short_times) >= 2:
                        # L·∫•y ng√†y g·ªëc c·ªßa Job c≈©
                        base_date = old_row['start_time'].strftime('%Y-%m-%d')
                        # Gh√©p ng√†y + gi·ªù t√¨m ƒë∆∞·ª£c
                        # L∆∞u √Ω: short_times c√≥ th·ªÉ b·∫Øt nh·∫ßm gi·ªù trong log message, n√™n l·∫•y 2 c√°i cu·ªëi
                        date_since = f"{base_date} {short_times[-2]}:00"
                        date_until = f"{base_date} {short_times[-1]}:00"
                        log(f"üîô RETRY V85 (Short): Reconstructed: {date_since} -> {date_until}")

            # [STRICT MODE] N·∫øu v·∫´n kh√¥ng t√¨m th·∫•y -> B√ÅO L·ªñI
            if not date_since:
                log(f"‚ùå RETRY FAILED: Cannot find time patterns in logs of Job #{retry_job_id}.")
                cur.execute("UPDATE job_history SET end_time=NOW(), status='Failed' WHERE id=%s", (hist_id,))
                conn.commit()
                return

        # --- CASE 2: DEMO / MANUAL ---
        if not date_since:
            now = datetime.now()
            delay_minutes = 45 if run_type == 'demo' else 90
            duration_minutes = 15 if run_type == 'demo' else 60
            
            end_dt = now - timedelta(minutes=delay_minutes)
            start_dt = end_dt - timedelta(minutes=duration_minutes)
            
            date_since = start_dt.strftime('%Y-%m-%d %H:%M:%S')
            date_until = end_dt.strftime('%Y-%m-%d %H:%M:%S')
            log(f"‚öôÔ∏è MANUAL MODE: {date_since} -> {date_until}")

        log(f" üïí Scanning Window: {date_since} -> {date_until}")
        
        # 4. G·ªåI API APPMETRICA (D√πng .strip() an to√†n)
        clean_app_id = str(app['app_id']).strip()
        clean_token = str(app['api_token']).strip()

        url = "https://api.appmetrica.yandex.com/logs/v1/export/events.json"
        params = {
            "application_id": clean_app_id,
            "date_since": date_since,
            "date_until": date_until,
            "fields": "event_name,event_timestamp,event_json",
            "limit": 1000000
        }
        headers = {"Authorization": f"OAuth {clean_token}"}
        
        status = "Failed"; total_events = 0
        
        for i in range(18): # 18 retries
            if stop_event.is_set():
                log("üõë USER STOPPED PROCESS.")
                status = "Cancelled"; break

            log(f"üì° Requesting AppMetrica (Attempt {i+1}/18)...")
            resp = requests.get(url, params=params, headers=headers)
            
            if resp.status_code == 200:
                data = resp.json().get('data', [])
                total_events = len(data)
                log(f"‚úÖ Success! Received {total_events} events. Importing...")
                
                conn_insert = get_db(); cur_insert = conn_insert.cursor()
                values = []
                for d in data:
                    try: ts = datetime.fromtimestamp(int(d.get('event_timestamp')))
                    except: ts = datetime.now()
                    values.append((app_id, d.get('event_name', 'unknown'), json.dumps(d), 1, ts))
                
                cur_insert.executemany("INSERT INTO event_logs (app_id, event_name, event_json, count, created_at) VALUES (%s,%s,%s,%s,%s)", values)
                conn_insert.commit(); conn_insert.close()
                
                status = "Success"; log(f"üéâ Done. Imported {total_events} events."); break
            
            elif resp.status_code == 202:
                log(f"‚è≥ Server 202. Waiting 180s..."); 
                if stop_event.wait(180): status = "Cancelled"; break
            else:
                log(f"‚ùå Error {resp.status_code}"); status = "Failed"; break
        
        # C·∫≠p nh·∫≠t k·∫øt qu·∫£ cu·ªëi c√πng
        conn_end = get_db(); cur_end = conn_end.cursor()
        cur_end.execute("UPDATE job_history SET end_time=NOW(), status=%s, total_events=%s WHERE id=%s", (status, total_events, hist_id))
        conn_end.commit(); conn_end.close()
    
    except Exception as e:
        log(f"‚ùå Critical Error: {str(e)}")
    finally:
        set_system_busy(False)

# PH·∫¶N 5: API ENDPOINTS (ƒê√É C·∫¨P NH·∫¨T DASHBOARD)
@app.route("/monitor/history", methods=['GET'])
def get_history():
    app_id = request.args.get('app_id') 
    # [M·ªöI] L·∫•y tham s·ªë ph√¢n trang
    try:
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 30)) # M·∫∑c ƒë·ªãnh 30 d√≤ng/trang
    except:
        page = 1; limit = 30
        
    offset = (page - 1) * limit
    conn = get_db()
    if not conn: return jsonify([])
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # 1. X√¢y d·ª±ng m·ªánh ƒë·ªÅ WHERE
        where_clause = ""
        params_count = []
        if app_id: 
            where_clause = "WHERE h.app_id = %s"
            params_count.append(app_id)

        # 2. ƒê·∫øm t·ªïng s·ªë records (ƒë·ªÉ t√≠nh s·ªë trang)
        cur.execute(f"SELECT COUNT(*) as total FROM job_history h {where_clause}", tuple(params_count))
        total_records = cur.fetchone()['total']
        total_pages = (total_records + limit - 1) // limit

        # 3. L·∫•y d·ªØ li·ªáu ph√¢n trang
        query = f"""
            SELECT h.*, a.name as app_name 
            FROM job_history h 
            JOIN apps a ON h.app_id = a.id 
            {where_clause}
            ORDER BY h.start_time DESC 
            LIMIT %s OFFSET %s
        """
        # Copy params t·ª´ count sang v√† th√™m limit/offset
        params_data = params_count + [limit, offset]
        
        cur.execute(query, tuple(params_data))
        res = cur.fetchall()

        # --- [FIX L·ªñI TIMEZONE V√Ä DURATION] ---
        # T√≠nh to√°n Duration v√† Format l·∫°i Time ƒë·ªÉ tr√°nh Frontend t·ª± c·ªông +7 ti·∫øng
        for row in res:
            # 1. T√≠nh Duration
            duration_str = "..."
            if row['start_time'] and row['end_time']:
                diff = row['end_time'] - row['start_time']
                total_seconds = int(diff.total_seconds())
                minutes = total_seconds // 60
                seconds = total_seconds % 60
                duration_str = f"{minutes} min {seconds} sec"
            elif row['status'] == 'Running':
                 duration_str = "Running..."
            
            row['duration'] = duration_str

            # 2. Fix Timezone: Chuy·ªÉn datetime th√†nh string c·ª©ng
            # ƒê·ªÉ Frontend hi·ªÉn th·ªã y nguy√™n gi·ªù c·ªßa Server (Local VN)
            if row['start_time']:
                row['start_time'] = row['start_time'].strftime('%d/%m/%Y %H:%M:%S')
            if row['end_time']:
                row['end_time'] = row['end_time'].strftime('%d/%m/%Y %H:%M:%S')

        return jsonify({
            "data": res,
            "pagination": {
                "current_page": page,
                "total_pages": total_pages,
                "total_records": total_records
            }
        })
    except Exception as e:
        print(f"History Error: {e}")
        return jsonify({"data": [], "pagination": {}})    
    finally: conn.close()

@app.route("/monitor/purge", methods=['DELETE'])
def purge_history():
    conn = get_db()
    try:
        cur = conn.cursor()
        cur.execute("DELETE FROM job_history")
        conn.commit()
        return jsonify({"msg": "History Cleared"})
    finally: conn.close()

@app.route("/apps", methods=['GET', 'POST'])
def handle_apps():
    conn = get_db()
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        if request.method == 'GET':
            cur.execute("SELECT * FROM apps ORDER BY id ASC")
            return jsonify(cur.fetchall())
        else:
            d = request.json
            cur.execute("INSERT INTO apps (name, app_id, api_token, is_active, schedule_time, interval_minutes) VALUES (%s, %s, %s, %s, %s, %s)", 
                        (d['name'], d['app_id'], d['api_token'], d['is_active'], d.get('schedule_time', '12:00'), d.get('interval_minutes', 60)))
            conn.commit()
            return jsonify({"msg": "Created"})
    finally: conn.close()

@app.route("/apps/<int:id>", methods=['PUT', 'DELETE'])
def update_app(id):
    conn = get_db()
    try:
        cur = conn.cursor()
        if request.method == 'PUT':
            d = request.json
            cur.execute("UPDATE apps SET name=%s, app_id=%s, api_token=%s, is_active=%s, schedule_time=%s, interval_minutes=%s WHERE id=%s", 
                        (d['name'], d['app_id'], d['api_token'], d['is_active'], d.get('schedule_time', '12:00'), d.get('interval_minutes', 60), id))
            conn.commit()
            return jsonify({"msg": "Updated"})
        elif request.method == 'DELETE':
            cur.execute("DELETE FROM event_logs WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM job_history WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM etl_jobs WHERE app_id=%s", (id,))
            cur.execute("DELETE FROM apps WHERE id=%s", (id,))
            conn.commit()
            return jsonify({"msg": "Deleted"})
    finally: conn.close()

@app.route("/etl/run/<int:app_id>", methods=['POST'])
def run_etl_api(app_id):
    # --- CODE M·ªöI ---
    data = request.json
    run_type = data.get('run_type', 'manual') # L·∫•y lo·∫°i ch·∫°y (manual/retry/demo)
    retry_job_id = data.get('retry_job_id')   # L·∫•y ID c·ªßa job c≈© n·∫øu l√† retry
    
    is_demo = (run_type == 'demo')
    
    if is_system_busy():
         return jsonify({"status": "error", "message": "System is busy processing another job. Please skip this cycle."}), 409

    # Truy·ªÅn th√™m retry_job_id v√†o h√†m x·ª≠ l√Ω
    threading.Thread(target=perform_manual_etl, args=(app_id, run_type, is_demo, retry_job_id)).start()
    return jsonify({"status": "started", "mode": run_type})

@app.route("/dashboard/<int:app_id>", methods=['GET'])
def get_dashboard(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB Connection failed"}), 500
    
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)

        # 1. KH·ªûI T·∫†O BI·∫æN AN TO√ÄN (FIX L·ªñI 'variable not defined')
        booster_config_list = [] 
        real_events = ["iapSuccess", "firstIAP", "iapPurchase"]
        start_events = ["missionStart", "missionStart_Daily", "level_start", "level_loading_start", "level_first_start"]
        fail_events = ["missionFail", "missionFail_Daily", "level_fail", "level_lose"]
        
        try:
            config = get_app_config(cur, app_id)
            if config:
                # C·∫≠p nh·∫≠t n·∫øu config c√≥ d·ªØ li·ªáu
                c_booster = config.get('boosters')
                if isinstance(c_booster, list): booster_config_list = c_booster
                
                c_real = config.get('events', {}).get('transaction', {}).get('real_currency')
                if c_real: real_events = c_real
        except: pass

        # L·∫•y danh s√°ch key booster
        booster_keys = [b['key'] for b in booster_config_list if isinstance(b, dict) and 'key' in b]
        if not booster_keys:
            booster_keys = ["booster_Hammer", "booster_Magnet", "booster_Add", "booster_Unlock", "booster_Clear", "revive_boosterClear", "booster_bubble", "booster_shuffle", "booster_ufo"]

        # 2. FILTER TIME
        where_clause = "WHERE app_id = %s"
        params = [app_id]
        if start_date: 
            where_clause += " AND created_at >= %s"
            params.append(start_date + " 00:00:00")
        if end_date: 
            where_clause += " AND created_at <= %s"
            params.append(end_date + " 23:59:59")

        # 3. DOANH THU (Regex Extract)
        cur.execute(r"""
            SELECT COALESCE(SUM(
                COALESCE(SUBSTRING(event_json FROM '"coin_spent"\s*:\s*"?(\d+)"?')::numeric, 0)
            ), 0)::int as real_revenue
            FROM event_logs
            """ + where_clause + r""" 
            AND event_name = ANY(%s) 
        """, tuple(params + [real_events]))
        real_revenue = cur.fetchone()['real_revenue']

        # 4. T·ªîNG TI√äU COIN (Regex Extract)
        cur.execute(r"""
            SELECT SUM(
                 COALESCE(SUBSTRING(event_json FROM '"coin_spent"\s*:\s*"?(\d+)"?')::int, 0)
            )::int as virtual_sink
            FROM event_logs
            """ + where_clause + r"""
            AND NOT (event_name = ANY(%s))
        """, tuple(params + [real_events]))
        virtual_sink = cur.fetchone()['virtual_sink'] or 0

        # 5. TOTAL PLAYS
        cur.execute(f"SELECT COUNT(*)::int as count FROM event_logs {where_clause} AND event_name = ANY(%s)", tuple(params + [start_events]))
        total_plays = cur.fetchone()['count'] or 0

        # 6. FAIL RATE
        cur.execute(f"SELECT COUNT(*)::int as count FROM event_logs {where_clause} AND event_name = ANY(%s)", tuple(params + [fail_events]))
        real_fail_count = cur.fetchone()['count'] or 0
        fail_rate = round((real_fail_count / total_plays) * 100, 1) if total_plays > 0 else 0.0

        # 7. CHART MAIN
        cur.execute(f"SELECT event_name as name, COUNT(*)::int as value FROM event_logs {where_clause} GROUP BY event_name ORDER BY value DESC", tuple(params))
        chart_data = cur.fetchall()

        # 8. BOOSTER REVENUE (Manual Count b·∫±ng SQL)
        # V√¨ Regex trong Group By ph·ª©c t·∫°p, ta l·∫•y raw v·ªÅ x·ª≠ l√Ω m·ªôt ch√∫t ho·∫∑c d√πng count ƒë∆°n gi·∫£n
        booster_stats = []
        if booster_keys:
            # Map gi√°
            PRICE_MAP = {}
            NAME_MAP = {}
            for b in booster_config_list:
                if isinstance(b, dict):
                    k = b.get('key')
                    PRICE_MAP[k] = b.get('price', 100)
                    NAME_MAP[k] = b.get('name', k)

            # Query ƒë·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa key trong chu·ªói JSON
            # L∆∞u √Ω: C√°ch n√†y ƒë·∫øm s·ªë d√≤ng c√≥ ch·ª©a key ƒë√≥
            for key in booster_keys:
                # Regex t√¨m "key": so_luong (s·ªë l∆∞·ª£ng > 0)
                # V√≠ d·ª•: "booster_Hammer": 1
                cur.execute(r"""
                    SELECT COUNT(*) as cnt 
                    FROM event_logs 
                    """ + where_clause + r""" 
                    AND event_json ~ (%s || '\s*:\s*[1-9]')
                """, tuple(params + [key]))
                
                count = cur.fetchone()['cnt']
                if count > 0:
                    price = PRICE_MAP.get(key, 100)
                    name = NAME_MAP.get(key, key)
                    booster_stats.append({
                        "name": name,
                        "value": count,
                        "revenue": count * price,
                        "price": price
                    })
            
            booster_stats.sort(key=lambda x: x['revenue'], reverse=True)

        return jsonify({
            "success": True,
            "overview": {
                "cards": {
                    "revenue": real_revenue,      
                    "active_users": total_plays,
                    "avg_fail_rate": fail_rate,
                    "total_spent": virtual_sink  
                },
                "chart_main": chart_data,
                "booster_chart": booster_stats
            }
        })

    except Exception as e:
        print(f"Error dashboard V93: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

@app.route("/api/levels/<int:app_id>", methods=['GET'])
def get_levels(app_id):
    conn = get_db()
    if not conn: return jsonify([])
    try:
        cur = conn.cursor()
        # L·∫•y TO√ÄN B·ªò d·ªØ li·ªáu ƒë·ªÉ l·ªçc ch√≠nh x√°c (Kh√¥ng d√πng LIMIT)
        cur.execute("SELECT event_json FROM event_logs WHERE app_id = %s", (app_id,))
        rows = cur.fetchall()
        
        levels = set()
        for r in rows:
            # D√πng m≈©i khoan v·∫°n nƒÉng V95
            data = universal_flatten(r[0])
            
            # T√¨m Level ID ·ªü m·ªçi key c√≥ th·ªÉ (Game 1 & 2)
            lvl = data.get('levelID') or data.get('level_display') or data.get('missionID')
            
            if lvl is not None:
                # L·ªçc l·∫•y s·ªë
                digits = ''.join(filter(str.isdigit, str(lvl)))
                if digits:
                    l = int(digits)
                    if l <= 2000: levels.add(l)
        
        sorted_levels = sorted(list(levels))
        return jsonify([str(l) for l in sorted_levels])
    except Exception as e:
        print(f"Error get_levels V95: {e}")
        return jsonify([])
    finally: conn.close()

@app.route("/dashboard/<int:app_id>/level-detail", methods=['GET'])
def get_level_detail(app_id):
    level_id = request.args.get('level_id') # Level ng∆∞·ªùi d√πng ch·ªçn t·ª´ menu
    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')
    
    try: page = int(request.args.get('page', 1)); limit = int(request.args.get('limit', 50))
    except: page=1; limit=50
    offset = (page - 1) * limit

    conn = get_db()
    if not conn: return jsonify({"success": False}), 500
    
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # Config Booster Map (Fallback)
        PRICE_MAP = {"Hammer": 120, "Magnet": 80, "Add": 60, "Unlock": 190, "Clear": 120}
        DISPLAY_MAP = {"Hammer": "Hammer üî®", "Magnet": "Magnet üß≤", "Add": "Add Moves ‚ûï"}
        
        # C·ªë g·∫Øng load config x·ªãn t·ª´ DB
        try:
            cfg = get_app_config(cur, app_id)
            if cfg and 'boosters' in cfg:
                for b in cfg['boosters']:
                    if isinstance(b, dict):
                        k = b.get('key',''); nm = b.get('name',''); pr = b.get('price', 100)
                        cl = k.replace('booster_', '').replace('revive_', '')
                        PRICE_MAP[cl] = pr; PRICE_MAP[k] = pr
                        DISPLAY_MAP[cl] = nm; DISPLAY_MAP[k] = nm
        except: pass

        # L·∫§Y TO√ÄN B·ªò LOG TRONG KHO·∫¢NG TH·ªúI GIAN (L·ªçc Level b·∫±ng Python cho ch·∫Øc)
        where = "WHERE app_id = %s"; params = [app_id]
        if start_date: where += " AND created_at >= %s"; params.append(start_date + " 00:00:00")
        if end_date: where += " AND created_at <= %s"; params.append(end_date + " 23:59:59")
        
        # S·∫Øp x·∫øp m·ªõi nh·∫•t tr∆∞·ªõc
        cur.execute(f"SELECT created_at, event_name, event_json FROM event_logs {where} ORDER BY created_at DESC", tuple(params))
        all_rows = cur.fetchall()

        # --- X·ª¨ L√ù PYTHON ---
        target_lvl = str(level_id)
        filtered_rows = []
        
        metrics = {"start":0, "win":0, "fail":0, "spend":0, "rev":0}
        b_counts = {}
        
        start_set = {"missionStart", "missionStart_Daily", "level_start", "level_loading_start"}
        win_set = {"missionComplete", "missionComplete_Daily", "level_win"}
        fail_set = {"missionFail", "missionFail_Daily", "level_fail", "level_lose"}

        for r in all_rows:
            # M≈®I KHOAN V95
            data = universal_flatten(r['event_json'])
            
            # Ki·ªÉm tra Level (Ch·∫•p nh·∫≠n c·∫£ s·ªë v√† chu·ªói)
            row_lvl = str(data.get('levelID') or data.get('level_display') or data.get('missionID') or "")
            if row_lvl != target_lvl: continue
            
            # N·∫øu kh·ªõp level -> L∆∞u l·∫°i ƒë·ªÉ x·ª≠ l√Ω ti·∫øp
            r_dict = dict(r); r_dict['parsed'] = data
            filtered_rows.append(r_dict)
            
            evt = r['event_name']
            if evt in start_set: metrics['start'] += 1
            elif evt in win_set: metrics['win'] += 1
            elif evt in fail_set: metrics['fail'] += 1
            
            # Ti·ªÅn
            money = int(data.get('coin_spent') or data.get('coin_cost') or 0)
            if money > 0:
                metrics['spend'] += 1
                metrics['rev'] += money
            
            # Booster (Qu√©t t·∫•t c·∫£ key)
            for k, v in data.items():
                if ('booster' in k or 'revive' in k) and str(v).isdigit() and int(v) > 0:
                    clean = k.replace('booster_', '').replace('revive_', '')
                    b_counts[clean] = b_counts.get(clean, 0) + int(v)

        # T√çNH TO√ÅN METRICS CU·ªêI C√ôNG
        real_plays = metrics['win'] + metrics['fail']
        if real_plays == 0: real_plays = metrics['start']
        win_rate = round((metrics['win']/real_plays)*100, 1) if real_plays > 0 else 0
        
        # Danh s√°ch Booster
        b_list = []
        for k, c in b_counts.items():
            nm = DISPLAY_MAP.get(k, k.capitalize())
            pr = PRICE_MAP.get(k, 100)
            b_list.append({"item_name": nm, "usage_count": c, "revenue": c*pr, "price": pr, "type": "Used"})
        b_list.sort(key=lambda x: x['revenue'], reverse=True)
        
        top_item = b_list[0]['item_name'] if b_list else "None"
        arpu = sum(x['revenue'] for x in b_list)

        final_metrics = { "total_plays": real_plays, "win_rate": win_rate, "arpu": arpu, "avg_balance": 0, "top_item": top_item }
        funnel = [
            {"event_type": "START", "count": real_plays, "revenue": 0},
            {"event_type": "WIN", "count": metrics['win'], "revenue": 0},
            {"event_type": "SPEND", "count": metrics['spend'], "revenue": metrics['rev']},
            {"event_type": "FAIL", "count": metrics['fail'], "revenue": 0}
        ]

        # PH√ÇN TRANG (Pagination)
        total_rec = len(filtered_rows)
        paged_data = filtered_rows[offset : offset + limit]
        
        proc_logs = []
        for r in paged_data:
            d = r['parsed']
            u = d.get('userID') or d.get('uuid') or "Guest"
            dt = []
            if d.get('coin_spent'): dt.append(f"üí∏ -{d['coin_spent']}")
            for k,v in d.items():
                if ('booster' in k or 'revive' in k) and int(v) > 0: 
                    dt.append(f"‚ö° {k.replace('booster_', '')} x{v}")
            
            proc_logs.append({
                "time": r['created_at'].strftime('%H:%M:%S %d/%m'),
                "user_id": str(u)[:15]+"..",
                "event_name": r['event_name'],
                "coin_spent": d.get('coin_spent', 0),
                "item_name": " | ".join(dt) if dt else "-"
            })

        return jsonify({
            "success": True,
            "metrics": final_metrics, "funnel": funnel, "booster_usage": b_list, 
            "logs": { "data": proc_logs, "pagination": { "current": page, "total_pages": (total_rec+limit-1)//limit, "total_records": total_rec } }
        })

    except Exception as e:
        print(f"Level Detail Error V95: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

@app.route("/dashboard/<int:app_id>/strategic", methods=['GET'])
def get_strategic_overview(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB error"}), 500

    start_date = request.args.get('start_date')
    end_date = request.args.get('end_date')

    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # 1. DANH S√ÅCH S·ª∞ KI·ªÜN T·ªîNG H·ª¢P (Game 1 + Game 2)
        start_set = {"missionStart", "missionStart_Daily", "level_start", "level_loading_start", "level_first_start"}
        fail_set = {"missionFail", "missionFail_Daily", "level_fail", "level_lose"}
        
        # 2. L·∫§Y D·ªÆ LI·ªÜU TH√î (FULL SCAN)
        where = "WHERE app_id = %s"; params = [app_id]
        if start_date: where += " AND created_at >= %s"; params.append(start_date + " 00:00:00")
        if end_date: where += " AND created_at <= %s"; params.append(end_date + " 23:59:59")
        
        # Ch·ªâ l·∫•y c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
        cur.execute(f"SELECT event_name, event_json FROM event_logs {where}", tuple(params))
        rows = cur.fetchall()
        
        # 3. T√çNH TO√ÅN (PYTHON AGGREGATION)
        stats = {} # {lvl: {plays, fails, rev}}

        for r in rows:
            # B∆Ø·ªöC QUAN TR·ªåNG: KHOAN S√ÇU D·ªÆ LI·ªÜU
            data = universal_flatten(r['event_json'])
            
            # T√¨m Level
            lvl_raw = data.get('levelID') or data.get('level_display') or data.get('missionID')
            if not lvl_raw: continue
            
            try:
                lvl_num = int(''.join(filter(str.isdigit, str(lvl_raw))))
                if lvl_num > 2000: continue
            except: continue
            
            if lvl_num not in stats: stats[lvl_num] = {"plays": 0, "fails": 0, "rev": 0}
            
            evt = r['event_name']
            if evt in start_set: stats[lvl_num]['plays'] += 1
            elif evt in fail_set: stats[lvl_num]['fails'] += 1
            
            # C·ªông ti·ªÅn (∆Øu ti√™n coin_spent, d·ª± ph√≤ng coin_cost)
            money = data.get('coin_spent') or data.get('coin_cost') or 0
            try: stats[lvl_num]['rev'] += float(money)
            except: pass

        # 4. FORMAT CHART
        chart = []
        for lvl, val in stats.items():
            if val['plays'] > 0 or val['rev'] > 0 or val['fails'] > 0:
                fr = round((val['fails'] / val['plays']) * 100, 1) if val['plays'] > 0 else 0
                if fr > 100: fr = 100.0
                chart.append({
                    "name": f"Lv.{lvl}", "level_index": lvl,
                    "revenue": val['rev'], "fail_rate": fr, "plays": val['plays']
                })

        chart.sort(key=lambda x: x['level_index'])
        return jsonify({"success": True, "balance_chart": chart})

    except Exception as e:
        print(f"Strategic Error V95: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

# --- B·ªî SUNG API: X√ìA 1 D√íNG & STOP JOB ---
@app.route("/monitor/history/<int:id>", methods=['DELETE'])
def delete_single_history(id):
    conn = get_db()
    try:
        cur = conn.cursor()
        cur.execute("DELETE FROM job_history WHERE id = %s", (id,))
        conn.commit()
        return jsonify({"success": True, "msg": f"Deleted history #{id}"})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

@app.route("/etl/stop/<int:hist_id>", methods=['POST'])
def stop_etl_process(hist_id):
    # API n√†y d√πng ƒë·ªÉ ƒë√°nh d·∫•u job l√† Cancelled tr√™n Database
    # N√≥ c≈©ng c·ªë g·∫Øng reset tr·∫°ng th√°i b·∫≠n c·ªßa h·ªá th·ªëng n·∫øu c·∫ßn
    conn = get_db()
    try:
        # 1. K√≠ch ho·∫°t c·ªù d·ª´ng ƒë·ªÉ Worker ƒëang ch·∫°y t·ª± tho√°t
        if hist_id in JOB_STOP_EVENTS:
            print(f"üõë Sending STOP signal to Job #{hist_id}...")
            JOB_STOP_EVENTS[hist_id].set() # ƒê√°nh th·ª©c Worker ngay l·∫≠p t·ª©c
        cur = conn.cursor()
        # C·∫≠p nh·∫≠t DB
        cur.execute("""
            UPDATE job_history 
            SET status = 'Cancelled', end_time = NOW(), logs = logs || E'\n[USER MANUAL STOP]'
            WHERE id = %s AND status IN ('Running', 'Processing')
        """, (hist_id,))
        conn.commit()

        # 3. Reset h·ªá th·ªëng n·∫øu c·∫ßn
        if is_system_busy():
            set_system_busy(False)
            
        return jsonify({"success": True, "msg": f"Stop signal sent to Job #{hist_id}"})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    finally: conn.close()

# --- [V41 FIX] API C·∫§U H√åNH ƒê·ªòNG (D√ôNG JSON DB) ---
@app.route("/apps/<int:app_id>/analytics-config", methods=['GET', 'POST'])
def handle_analytics_config(app_id):
    conn = get_db()
    if not conn: return jsonify({"success": False, "error": "DB Connection Failed"}), 500
    
    try:
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # --- 1. L·∫§Y C·∫§U H√åNH (GET) ---
        if request.method == 'GET':
            # Query m·ªõi: Ch·ªâ l·∫•y c·ªôt config_json
            cur.execute("SELECT config_json FROM analytics_config WHERE app_id = %s", (app_id,))
            row = cur.fetchone()
            
            if row and row['config_json']:
                # Tr·∫£ v·ªÅ JSON chu·∫©n cho Frontend
                return jsonify(row['config_json'])
            else:
                # N·∫øu ch∆∞a c√≥ trong DB, tr·∫£ v·ªÅ config m·∫∑c ƒë·ªãnh t·ª´ h√†m get_app_config
                # (L∆∞u √Ω: B·∫°n ph·∫£i ƒë·∫£m b·∫£o h√†m get_app_config ·ªü ƒë·∫ßu file ƒë√£ s·ª≠a t√™n b·∫£ng th√†nh analytics_config nh√©)
                return jsonify(get_app_config(cur, app_id))

        # --- 2. L∆ØU C·∫§U H√åNH (POST) ---
        elif request.method == 'POST':
            new_config = request.json # Frontend g·ª≠i l√™n to√†n b·ªô c·ª•c JSON settings
            
            # Chuy·ªÉn Dict th√†nh String JSON ƒë·ªÉ l∆∞u v√†o DB
            config_str = json.dumps(new_config)

            # L∆∞u th·∫≥ng v√†o c·ªôt config_json (G·ªçn nh·∫π h∆°n logic c≈© r·∫•t nhi·ªÅu)
            cur.execute("""
                INSERT INTO analytics_config (app_id, config_json, updated_at)
                VALUES (%s, %s, NOW())
                ON CONFLICT (app_id) DO UPDATE SET 
                    config_json = EXCLUDED.config_json,
                    updated_at = NOW()
            """, (app_id, config_str))
            
            conn.commit()
            return jsonify({"success": True, "msg": "Configuration Saved Successfully (V40 JSON Mode)"})
            
    except Exception as e:
        print(f"‚ùå Analytics Config Error: {e}")
        conn.rollback() # Ch·ªëng k·∫πt transaction
        return jsonify({"success": False, "error": str(e)}), 500
    finally:
        conn.close()

# --- API CH·∫†Y ETL (T·ªîNG H·ª¢P D·ªÆ LI·ªÜU) ---
@app.route("/api/run-etl/<int:app_id>", methods=['POST'])
def trigger_etl_process(app_id):
    # Ch·∫°y trong thread ri√™ng ƒë·ªÉ kh√¥ng block server
    threading.Thread(target=run_etl_pipeline, args=(app_id,)).start()
    return jsonify({"status": "started", "message": "ETL process started in background"})

# --- API M·ªöI: TRA C·ª®U D·ªÆ LI·ªÜU TH√î (DATA EXPLORER) ---
@app.route("/events/search", methods=['GET'])
def search_events():
    try:
        app_id = request.args.get('app_id')
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 20))
        
        # C√°c b·ªô l·ªçc
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        event_name = request.args.get('event_name')
        keyword = request.args.get('keyword') # T√¨m UserID ho·∫∑c n·ªôi dung b·∫•t k·ª≥ trong JSON

        if not app_id:
            return jsonify({"success": False, "error": "Missing app_id"}), 400

        conn = get_db()
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        # 1. X√¢y d·ª±ng c√¢u WHERE ƒë·ªông
        where_clauses = ["app_id = %s"]
        params = [app_id]

        if start_date:
            where_clauses.append("created_at >= %s")
            params.append(start_date + " 00:00:00")
        
        if end_date:
            where_clauses.append("created_at <= %s")
            params.append(end_date + " 23:59:59")

        if event_name and event_name.strip():
            where_clauses.append("event_name = %s")
            params.append(event_name)

        if keyword and keyword.strip():
            # K·ªπ thu·∫≠t t√¨m ki·∫øm trong JSON (Chuy·ªÉn JSON th√†nh Text ƒë·ªÉ t√¨m)
            where_clauses.append("event_json::text ILIKE %s")
            params.append(f"%{keyword}%")

        full_where = " WHERE " + " AND ".join(where_clauses)

        # 2. ƒê·∫øm t·ªïng s·ªë d√≤ng (ƒë·ªÉ l√†m ph√¢n trang 1/100...)
        count_query = f"SELECT COUNT(*) as total FROM event_logs {full_where}"
        cursor.execute(count_query, tuple(params))
        total_records = cursor.fetchone()['total']
        total_pages = (total_records + limit - 1) // limit

        # 3. L·∫•y d·ªØ li·ªáu ph√¢n trang
        offset = (page - 1) * limit
        data_query = f"""
            SELECT 
                id, 
                event_name, 
                to_char(created_at, 'YYYY-MM-DD HH24:MI:SS') as created_at,
                event_json -- L·∫•y nguy√™n c·ª•c JSON v·ªÅ ƒë·ªÉ Frontend hi·ªÉn th·ªã ƒë·∫πp
            FROM event_logs 
            {full_where}
            ORDER BY created_at DESC
            LIMIT %s OFFSET %s
        """
        # Th√™m limit/offset v√†o params
        params.extend([limit, offset])
        
        cursor.execute(data_query, tuple(params))
        rows = cursor.fetchall()

        # 4. Tr√≠ch xu·∫•t s∆° b·ªô User ID ƒë·ªÉ hi·ªÉn th·ªã ra ngo√†i b·∫£ng (cho ti·ªán nh√¨n)
        for row in rows:
            try:
                # Parse JSON string th√†nh Dict
                import json
                raw = row['event_json']
                # X·ª≠ l√Ω double-encoded n·∫øu c√≥
                if isinstance(raw, str):
                    parsed = json.loads(raw)
                    # N·∫øu b√™n trong l·∫°i c√≥ key 'event_json' d·∫°ng string
                    if isinstance(parsed, dict) and 'event_json' in parsed and isinstance(parsed['event_json'], str):
                        inner = json.loads(parsed['event_json'])
                        parsed.update(inner)
                    row['event_json'] = parsed # G√°n l·∫°i object ƒë√£ s·∫°ch
                
                # --- [LOGIC M·ªöI] T·∫†O C·ªòT KEY INFO ---
                # Thay v√¨ l·∫•y UserID, ta l·∫•y th√¥ng tin ng·ªØ c·∫£nh quan tr·ªçng h∆°n
                data = row['event_json']
                info_parts = []
                
                # 1. N·∫øu c√≥ th√¥ng tin Level/Mission -> L·∫•y ngay
                if 'levelID' in data: info_parts.append(f"Lv.{data['levelID']}")
                if 'missionID' in data: info_parts.append(f"Ms.{data['missionID']}")
                
                # 2. N·∫øu c√≥ th√¥ng tin Ti·ªÅn/Gi√° -> L·∫•y ngay
                if 'coin_cost' in data: info_parts.append(f"-{data['coin_cost']} Coin")
                if 'coin_price' in data: info_parts.append(f"-{data['coin_price']} Coin")
                if 'revenue' in data: info_parts.append(f"+{data['revenue']} USD")
                
                # 3. N·∫øu c√≥ th√¥ng tin Item/Booster
                if 'item_name' in data: info_parts.append(data['item_name'])
                
                # G√°n v√†o bi·∫øn m·ªõi ƒë·ªÉ tr·∫£ v·ªÅ Frontend
                row['key_info'] = " | ".join(info_parts) if info_parts else "..."
            except:
                row['key_info'] = '-'
                row['event_json'] = {}

        return jsonify({
            "success": True,
            "data": rows,
            "pagination": {
                "current_page": page,
                "total_pages": total_pages,
                "total_records": total_records,
                "limit": limit
            }
        })

    except Exception as e:
        print(f"Search Error: {e}")
        return jsonify({"success": False, "error": str(e)}), 500
    finally:
        conn.close()

if __name__ == '__main__':
    t1 = threading.Thread(target=run_scheduler_loop)
    t1.daemon = True
    t1.start()

    t2 = threading.Thread(target=run_worker_loop)
    t2.daemon = True
    t2.start()

    print("üöÄ SYSTEM READY: Smart Scheduler & Worker Threads started...")
    app.run(host='0.0.0.0', port=8080, debug=True, use_reloader=False)